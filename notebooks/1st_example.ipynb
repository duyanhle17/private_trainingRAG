{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "081f1c62",
   "metadata": {},
   "source": [
    "SET UP môi trường nhé hẹ hẹ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "332cf19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (5.2.2)\n",
      "Requirement already satisfied: faiss-cpu in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: pandas in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from sentence-transformers) (5.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from sentence-transformers) (1.3.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from sentence-transformers) (2.10.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from sentence-transformers) (1.8.0)\n",
      "Requirement already satisfied: scipy in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from sentence-transformers) (1.17.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: tqdm in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (3.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2026.1.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.5.4)\n",
      "Requirement already satisfied: anyio in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.10.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from typer-slim->transformers<6.0.0,>=4.41.0->sentence-transformers) (8.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers faiss-cpu networkx pandas numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7db357",
   "metadata": {},
   "source": [
    "Load Json luật + Flatten tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f2a10f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from typing import Any, Dict, List, Optional\n",
    "\n",
    "# def norm(s: Optional[str]) -> str:\n",
    "#     return (s or \"\").replace(\"\\r\", \"\").strip()\n",
    "\n",
    "# def join_path(parts: List[str]) -> str:\n",
    "#     return \" > \".join([p for p in parts if p])\n",
    "\n",
    "# def flatten_legal_tree(node: Dict[str, Any], path: List[str], out: List[Dict[str, Any]]):\n",
    "#     ntype = node.get(\"type\", \"\")\n",
    "#     title = norm(node.get(\"title\"))\n",
    "#     content = norm(node.get(\"content\"))\n",
    "\n",
    "#     new_path = path + ([title] if title else [])\n",
    "#     if content:\n",
    "#         out.append({\"type\": ntype, \"path\": join_path(new_path), \"text\": content})\n",
    "\n",
    "#     for ch in node.get(\"children\", []) or []:\n",
    "#         flatten_legal_tree(ch, new_path, out)\n",
    "\n",
    "# def load_legal_json(json_path: str) -> Dict[str, Any]:\n",
    "#     with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         return json.load(f)\n",
    "\n",
    "# def flatten_to_passages(doc: Dict[str, Any]) -> List[str]:\n",
    "#     info = doc.get(\"document_info\", {})\n",
    "#     base = [\n",
    "#         f\"Văn bản: {norm(info.get('title'))}\",\n",
    "#         f\"URL: {norm(info.get('url'))}\",\n",
    "#         f\"Crawled: {norm(info.get('crawled_at'))}\",\n",
    "#     ]\n",
    "\n",
    "#     units: List[Dict[str, Any]] = []\n",
    "#     for top in doc.get(\"body\", []) or []:\n",
    "#         flatten_legal_tree(top, base, units)\n",
    "\n",
    "#     passages = []\n",
    "#     for u in units:\n",
    "#         header = f\"[{u['type']}] {u['path']}\"\n",
    "#         passages.append(header + \"\\n\" + u[\"text\"])\n",
    "#     return passages\n",
    "\n",
    "# JSON_PATH = \"thongtu80.json\"  # đổi path\n",
    "# doc = load_legal_json(JSON_PATH)\n",
    "# passages = flatten_to_passages(doc)\n",
    "\n",
    "# print(\"passages:\", len(passages))\n",
    "# print(passages[0][:800])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ade43e6",
   "metadata": {},
   "source": [
    "Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c30f35fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install tiktoken\n",
    "\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8efe351",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# class TokenSplitter:\n",
    "#     def __init__(self, chunk_tokens=800, overlap_tokens=80, tokenizer_name=\"cl100k_base\"):\n",
    "#         self.enc = tiktoken.get_encoding(tokenizer_name)\n",
    "#         self.chunk_tokens = chunk_tokens\n",
    "#         self.overlap_tokens = overlap_tokens\n",
    "\n",
    "#     def chunk(self, text: str):\n",
    "#         toks = self.enc.encode(text)\n",
    "#         step = max(1, self.chunk_tokens - self.overlap_tokens)\n",
    "#         out = []\n",
    "#         for i in range(0, len(toks), step):\n",
    "#             sub = toks[i:i+self.chunk_tokens]\n",
    "#             if sub:\n",
    "#                 out.append(self.enc.decode(sub))\n",
    "#         return out\n",
    "\n",
    "# splitter = TokenSplitter(chunk_tokens=800, overlap_tokens=80)\n",
    "\n",
    "# dataset = []\n",
    "# for p in passages:\n",
    "#     dataset.extend(splitter.chunk(p))\n",
    "\n",
    "# print(\"chunks:\", len(dataset))\n",
    "# print(dataset[0][:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a06e8f9",
   "metadata": {},
   "source": [
    "Embedding tiếng Việt (E5 multilingual) + FAISS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7e98989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import faiss\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# embed_model_name = \"intfloat/multilingual-e5-base\"  # hoặc multilingual-e5-large\n",
    "# embedder = SentenceTransformer(embed_model_name)\n",
    "\n",
    "# def embed_texts(texts, batch_size=32):\n",
    "#     # E5: prefix passage:\n",
    "#     inputs = [f\"passage: {t}\" for t in texts]\n",
    "#     vecs = embedder.encode(inputs, batch_size=batch_size, normalize_embeddings=True)\n",
    "#     return np.asarray(vecs, dtype=\"float32\")\n",
    "\n",
    "# embeddings = embed_texts(dataset, batch_size=32)\n",
    "# print(\"embeddings:\", embeddings.shape)\n",
    "\n",
    "# d = embeddings.shape[1]\n",
    "# index = faiss.IndexFlatIP(d)\n",
    "# index.add(embeddings)\n",
    "# print(\"FAISS added:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b202fc8",
   "metadata": {},
   "source": [
    "NANOVectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f21c51b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install nano-vectordb\n",
    "\n",
    "# from nano_vectordb import NanoVectorDB\n",
    "\n",
    "# vdb = NanoVectorDB(d, storage_file=\"vdb_chunks.json\")\n",
    "\n",
    "# # upsert\n",
    "# datas = []\n",
    "# for i, (text, vec) in enumerate(zip(dataset, embeddings)):\n",
    "#     datas.append({\"__id__\": f\"chunk-{i}\", \"__vector__\": vec, \"content\": text})\n",
    "\n",
    "# vdb.upsert(datas=datas)\n",
    "# vdb.save()\n",
    "# print(\"saved vdb_chunks.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e401c08f",
   "metadata": {},
   "source": [
    "LLM extractor (JSON schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eb5f918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import networkx as nx\n",
    "\n",
    "# def llm_extract_entities_relations(chunk: str) -> Dict[str, Any]:\n",
    "#     prompt = f\"\"\"\n",
    "# Bạn là hệ thống trích xuất tri thức cho văn bản luật Việt Nam.\n",
    "\n",
    "# Từ đoạn văn sau, hãy trích xuất:\n",
    "# - entities: danh sách thực thể quan trọng (cơ quan, đối tượng, khái niệm, văn bản pháp luật, thủ tục, hành vi, nghĩa vụ...)\n",
    "# - relations: danh sách quan hệ dạng (head, relation, tail)\n",
    "\n",
    "# QUY TẮC:\n",
    "# - Ưu tiên các khái niệm pháp lý: \"người nộp thuế\", \"cơ quan thuế\", \"hồ sơ hoàn thuế\", \"thương mại điện tử\", \"nền tảng số\"...\n",
    "# - Hạn chế tạo entity kiểu \"Điều 7\", \"khoản 1\" (chỉ giữ nếu liên quan trực tiếp).\n",
    "# - relation nên là động từ/cụm ngắn: \"được hiểu là\", \"bao gồm\", \"quy định\", \"hướng dẫn\", \"quản lý\", \"áp dụng cho\"...\n",
    "# - Output phải là JSON thuần theo format:\n",
    "\n",
    "# {{\n",
    "#   \"entities\": [\"...\"],\n",
    "#   \"relations\": [\n",
    "#     {{\"head\":\"...\", \"relation\":\"...\", \"tail\":\"...\"}}\n",
    "#   ]\n",
    "# }}\n",
    "\n",
    "# ĐOẠN VĂN:\n",
    "# \\\"\\\"\\\"{chunk}\\\"\\\"\\\"\n",
    "# JSON:\n",
    "# \"\"\"\n",
    "#     text = llm_generate(prompt, max_tokens=900)\n",
    "#     # cố parse JSON robust\n",
    "#     try:\n",
    "#         obj = json.loads(text)\n",
    "#         if \"entities\" not in obj: obj[\"entities\"] = []\n",
    "#         if \"relations\" not in obj: obj[\"relations\"] = []\n",
    "#         return obj\n",
    "#     except Exception:\n",
    "#         # fallback: cố lấy json block\n",
    "#         m = re.search(r\"\\{.*\\}\", text, flags=re.S)\n",
    "#         if m:\n",
    "#             try:\n",
    "#                 return json.loads(m.group(0))\n",
    "#             except Exception:\n",
    "#                 pass\n",
    "#         return {\"entities\": [], \"relations\": []}\n",
    "\n",
    "# # build KG\n",
    "# kg = nx.DiGraph()\n",
    "# chunk_entities = []\n",
    "\n",
    "# for i, ch in enumerate(dataset[:200]):  # test 200 chunk trước cho nhanh\n",
    "#     extracted = llm_extract_entities_relations(ch)\n",
    "#     ents = set([e.strip() for e in extracted.get(\"entities\", []) if e and isinstance(e, str)])\n",
    "#     chunk_entities.append(ents)\n",
    "\n",
    "#     for e in ents:\n",
    "#         if not kg.has_node(e):\n",
    "#             kg.add_node(e)\n",
    "\n",
    "#     for r in extracted.get(\"relations\", []) or []:\n",
    "#         h = (r.get(\"head\") or \"\").strip()\n",
    "#         rel = (r.get(\"relation\") or \"\").strip()\n",
    "#         t = (r.get(\"tail\") or \"\").strip()\n",
    "#         if h and t:\n",
    "#             kg.add_edge(h, t, relation=rel or \"related_to\", source_chunk=i)\n",
    "\n",
    "# print(\"KG nodes:\", kg.number_of_nodes())\n",
    "# print(\"KG edges:\", kg.number_of_edges())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "775971a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# set key ở đây (đừng hardcode trong code dài)\n",
    "os.environ[\"NVAPI_KEY\"] = \"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\"  # <-- điền key của bạn\n",
    "\n",
    "JSON_PATH = \"thongtu80.json\"\n",
    "CACHE_DIR = \"./tt80_cache_full\"   # thư mục lưu mọi thứ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45a015fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (2.16.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from openai) (2.12.5)\n",
      "Requirement already satisfied: sniffio in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: colorama in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e369d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: intfloat/multilingual-e5-base\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/multilingual-e5-base/835193815a3936a24a0ee7dc9e3d48c1fbb19c55/modules.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/config_sentence_transformers.json \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/config_sentence_transformers.json \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/multilingual-e5-base/835193815a3936a24a0ee7dc9e3d48c1fbb19c55/README.md \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/multilingual-e5-base/835193815a3936a24a0ee7dc9e3d48c1fbb19c55/modules.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/sentence_bert_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/multilingual-e5-base/835193815a3936a24a0ee7dc9e3d48c1fbb19c55/sentence_bert_config.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/adapter_config.json \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/multilingual-e5-base/835193815a3936a24a0ee7dc9e3d48c1fbb19c55/config.json \"HTTP/1.1 200 OK\"\n",
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 952.29it/s, Materializing param=pooler.dense.weight]                               \n",
      "XLMRobertaModel LOAD REPORT from: intfloat/multilingual-e5-base\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/multilingual-e5-base/835193815a3936a24a0ee7dc9e3d48c1fbb19c55/config.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/multilingual-e5-base/835193815a3936a24a0ee7dc9e3d48c1fbb19c55/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/intfloat/multilingual-e5-base/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/intfloat/multilingual-e5-base/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/intfloat/multilingual-e5-base \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/1_Pooling/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/multilingual-e5-base/835193815a3936a24a0ee7dc9e3d48c1fbb19c55/1_Pooling%2Fconfig.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/intfloat/multilingual-e5-base \"HTTP/1.1 200 OK\"\n",
      "INFO:OFFLINE_BUILD_NO_LLM:Load JSON -> passages\n",
      "INFO:OFFLINE_BUILD_NO_LLM:Chunking\n",
      "INFO:OFFLINE_BUILD_NO_LLM:Chunking mode: tiktoken\n",
      "INFO:OFFLINE_BUILD_NO_LLM:chunks = 1267\n",
      "INFO:OFFLINE_BUILD_NO_LLM:Embedding + FAISS\n",
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.02s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.31s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.06s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.62s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.73s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.93s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.01s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.16s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.11s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.36s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.07s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.19s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.78s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.58s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.06s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.86s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.88s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.83s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.48s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.87s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.32s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.42s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.48s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.47s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.67s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  7.00s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.82s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.04s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.53s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.58s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.98s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.01s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.86s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.92s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.65s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.96s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.96s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.55s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.37s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.30s/it]\n",
      "INFO:OFFLINE_BUILD_NO_LLM:FAISS ntotal = 1267\n",
      "INFO:OFFLINE_BUILD_NO_LLM:Build KG + chunk_entities (NO LLM)\n",
      "INFO:OFFLINE_BUILD_NO_LLM:KG progress 200/1267 | nodes=438 edges=1221\n",
      "INFO:OFFLINE_BUILD_NO_LLM:KG progress 400/1267 | nodes=674 edges=2098\n",
      "INFO:OFFLINE_BUILD_NO_LLM:KG progress 600/1267 | nodes=780 edges=2557\n",
      "INFO:OFFLINE_BUILD_NO_LLM:KG progress 800/1267 | nodes=806 edges=2679\n",
      "INFO:OFFLINE_BUILD_NO_LLM:KG progress 1000/1267 | nodes=837 edges=2780\n",
      "INFO:OFFLINE_BUILD_NO_LLM:KG progress 1200/1267 | nodes=1122 edges=3859\n",
      "INFO:OFFLINE_BUILD_NO_LLM:✅ DONE. Saved artifacts to: ./tt80_artifacts_no_llm\n",
      "INFO:OFFLINE_BUILD_NO_LLM:KG nodes=1261 edges=4426\n"
     ]
    }
   ],
   "source": [
    "import os, json, re, logging\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Optional, Set, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"OFFLINE_BUILD_NO_LLM\")\n",
    "\n",
    "# ====== CONFIG ======\n",
    "JSON_PATH  = \"thongtu80.json\"\n",
    "CACHE_DIR  = \"./tt80_artifacts_no_llm\"\n",
    "\n",
    "EMBED_MODEL = \"intfloat/multilingual-e5-base\"\n",
    "BATCH_EMBED = 32\n",
    "\n",
    "CHUNK_TOKENS = 800\n",
    "CHUNK_OVERLAP = 80\n",
    "CHUNK_CHAR = 1200\n",
    "CHUNK_CHAR_OVERLAP = 120\n",
    "\n",
    "# KG params (offline)\n",
    "TOP_K_TERMS_PER_CHUNK = 18\n",
    "MIN_TERM_LEN = 3\n",
    "MAX_TERM_WORDS = 6\n",
    "COOC_WINDOW = 2  # connect term i with i+1..i+COOC_WINDOW in the same chunk\n",
    "\n",
    "# ==============\n",
    "# Utils: Flatten legal JSON\n",
    "# ==============\n",
    "def norm(s: Optional[str]) -> str:\n",
    "    return (s or \"\").replace(\"\\r\", \"\").strip()\n",
    "\n",
    "def join_path(parts: List[str]) -> str:\n",
    "    return \" > \".join([p for p in parts if p])\n",
    "\n",
    "def flatten_legal_tree(node: Dict[str, Any], path: List[str], out: List[Dict[str, Any]]):\n",
    "    ntype = node.get(\"type\", \"\")\n",
    "    title = norm(node.get(\"title\"))\n",
    "    content = norm(node.get(\"content\"))\n",
    "    new_path = path + ([title] if title else [])\n",
    "    if content:\n",
    "        out.append({\"type\": ntype, \"path\": join_path(new_path), \"text\": content})\n",
    "    for ch in node.get(\"children\", []) or []:\n",
    "        flatten_legal_tree(ch, new_path, out)\n",
    "\n",
    "def load_legal_json(json_path: str) -> Dict[str, Any]:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def flatten_to_passages(doc: Dict[str, Any]) -> List[str]:\n",
    "    info = doc.get(\"document_info\", {})\n",
    "    base = [\n",
    "        f\"Văn bản: {norm(info.get('title'))}\",\n",
    "        f\"URL: {norm(info.get('url'))}\",\n",
    "        f\"Crawled: {norm(info.get('crawled_at'))}\",\n",
    "    ]\n",
    "    units: List[Dict[str, Any]] = []\n",
    "    for top in doc.get(\"body\", []) or []:\n",
    "        flatten_legal_tree(top, base, units)\n",
    "\n",
    "    passages = []\n",
    "    for u in units:\n",
    "        header = f\"[{u['type']}] {u['path']}\"\n",
    "        passages.append(header + \"\\n\" + u[\"text\"])\n",
    "    return passages\n",
    "\n",
    "# ==============\n",
    "# Chunking (token if tiktoken exists else char)\n",
    "# ==============\n",
    "def try_import_tiktoken():\n",
    "    try:\n",
    "        import tiktoken\n",
    "        return tiktoken\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def token_chunk(text: str, chunk_tokens: int, overlap: int, tokenizer_name=\"cl100k_base\") -> List[str]:\n",
    "    tiktoken = try_import_tiktoken()\n",
    "    if tiktoken is None:\n",
    "        raise ModuleNotFoundError(\"tiktoken not installed\")\n",
    "    enc = tiktoken.get_encoding(tokenizer_name)\n",
    "    toks = enc.encode(text)\n",
    "    step = max(1, chunk_tokens - overlap)\n",
    "    out = []\n",
    "    for i in range(0, len(toks), step):\n",
    "        sub = toks[i:i+chunk_tokens]\n",
    "        if sub:\n",
    "            out.append(enc.decode(sub))\n",
    "    return out\n",
    "\n",
    "def char_chunk(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
    "    seps = [\"\\n\\n\", \"\\n\", \". \", \"; \", \", \", \" \", \"\"]\n",
    "    chunks: List[str] = []\n",
    "    buf = text.strip()\n",
    "    if not buf:\n",
    "        return []\n",
    "    start = 0\n",
    "    while start < len(buf):\n",
    "        end = min(len(buf), start + chunk_size)\n",
    "        piece = buf[start:end]\n",
    "        cut = -1\n",
    "        for sep in seps:\n",
    "            idx = piece.rfind(sep)\n",
    "            if idx > cut:\n",
    "                cut = idx\n",
    "        if cut <= 0:\n",
    "            cut = len(piece)\n",
    "        else:\n",
    "            cut = cut + 1\n",
    "        chunk = piece[:cut].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        start = start + max(1, cut - overlap)\n",
    "    return chunks\n",
    "\n",
    "def make_chunks(passages: List[str]) -> List[str]:\n",
    "    dataset: List[str] = []\n",
    "    use_token = try_import_tiktoken() is not None\n",
    "    logger.info(f\"Chunking mode: {'tiktoken' if use_token else 'char-fallback'}\")\n",
    "    for p in passages:\n",
    "        if use_token:\n",
    "            try:\n",
    "                dataset.extend(token_chunk(p, CHUNK_TOKENS, CHUNK_OVERLAP))\n",
    "            except Exception:\n",
    "                dataset.extend(char_chunk(p, CHUNK_CHAR, CHUNK_CHAR_OVERLAP))\n",
    "        else:\n",
    "            dataset.extend(char_chunk(p, CHUNK_CHAR, CHUNK_CHAR_OVERLAP))\n",
    "    return dataset\n",
    "\n",
    "# ==============\n",
    "# Embedding + FAISS\n",
    "# ==============\n",
    "embedder = SentenceTransformer(EMBED_MODEL)\n",
    "\n",
    "def embed_texts(texts: List[str], batch_size=32) -> np.ndarray:\n",
    "    vecs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inp = [f\"passage: {t}\" for t in batch]\n",
    "        v = embedder.encode(inp, normalize_embeddings=True)\n",
    "        vecs.append(np.asarray(v, dtype=\"float32\"))\n",
    "    return np.vstack(vecs)\n",
    "\n",
    "def build_faiss(embeddings: np.ndarray) -> faiss.Index:\n",
    "    d = embeddings.shape[1]\n",
    "    idx = faiss.IndexFlatIP(d)\n",
    "    idx.add(embeddings.astype(\"float32\"))\n",
    "    return idx\n",
    "\n",
    "# ==============\n",
    "# OFFLINE Entity / Term extraction (NO LLM)\n",
    "# ==============\n",
    "def normalize_term(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    # remove surrounding quotes\n",
    "    s = s.strip(\"“”\\\"' \")\n",
    "    return s\n",
    "\n",
    "def extract_quoted_terms(text: str) -> List[str]:\n",
    "    # captures “...”, \"...\", '...'\n",
    "    terms = []\n",
    "    for a,b,c in re.findall(r\"“([^”]{2,120})”|\\\"([^\\\"]{2,120})\\\"|'([^']{2,120})'\", text):\n",
    "        t = normalize_term(a or b or c)\n",
    "        if t:\n",
    "            terms.append(t)\n",
    "    return terms\n",
    "\n",
    "def extract_legal_markers(text: str) -> List[str]:\n",
    "    # keep lawy concepts + headings\n",
    "    # examples: Điều 1, Khoản 2, điểm a, Nghị định số 126/2020/NĐ-CP\n",
    "    pats = [\n",
    "        r\"(Điều\\s+\\d+[A-Za-z]?)\",\n",
    "        r\"(Khoản\\s+\\d+[A-Za-z]?)\",\n",
    "        r\"(điểm\\s+[a-zđ])\",\n",
    "        r\"(Nghị\\s+định\\s+số\\s+\\d+\\/\\d+\\/[A-ZĐ\\-]+)\",\n",
    "        r\"(Thông\\s+tư\\s+\\d+\\/\\d+\\/[A-ZĐ\\-]+)\",\n",
    "        r\"(Luật\\s+[A-ZÀ-Ỹa-zà-ỹ\\s]{3,80})\",\n",
    "    ]\n",
    "    out = []\n",
    "    for pat in pats:\n",
    "        out.extend([normalize_term(x) for x in re.findall(pat, text, flags=re.I)])\n",
    "    return out\n",
    "\n",
    "def extract_terms_yake(text: str, top_k: int) -> List[str]:\n",
    "    try:\n",
    "        import yake\n",
    "        kw_extractor = yake.KeywordExtractor(lan=\"vi\", n=MAX_TERM_WORDS, top=top_k)\n",
    "        kws = kw_extractor.extract_keywords(text)\n",
    "        # yake returns list[(kw, score)] lower score better\n",
    "        kws = sorted(kws, key=lambda x: x[1])[:top_k]\n",
    "        return [normalize_term(k) for k,_ in kws if k]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def extract_terms_underthesea(text: str, top_k: int) -> List[str]:\n",
    "    # optional: better Vietnamese word segmentation + noun phrases\n",
    "    try:\n",
    "        from underthesea import pos_tag\n",
    "        tags = pos_tag(text)\n",
    "        # collect sequences of N / NP / A (rough)\n",
    "        terms = []\n",
    "        buf = []\n",
    "        for w,t in tags:\n",
    "            if t in (\"N\", \"Np\", \"Ny\", \"Nc\", \"A\"):\n",
    "                buf.append(w)\n",
    "            else:\n",
    "                if 1 <= len(buf) <= MAX_TERM_WORDS:\n",
    "                    terms.append(normalize_term(\" \".join(buf)))\n",
    "                buf = []\n",
    "        if 1 <= len(buf) <= MAX_TERM_WORDS:\n",
    "            terms.append(normalize_term(\" \".join(buf)))\n",
    "        # filter length\n",
    "        terms = [x for x in terms if len(x) >= MIN_TERM_LEN]\n",
    "        # unique preserve order\n",
    "        seen = set()\n",
    "        out = []\n",
    "        for x in terms:\n",
    "            lx = x.lower()\n",
    "            if lx not in seen:\n",
    "                seen.add(lx)\n",
    "                out.append(x)\n",
    "        return out[:top_k]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def extract_terms_offline(text: str, top_k: int = TOP_K_TERMS_PER_CHUNK) -> List[str]:\n",
    "    # Priority order: quoted terms + legal markers + underthesea + yake + fallback regex ngrams\n",
    "    terms = []\n",
    "    terms.extend(extract_quoted_terms(text))\n",
    "    terms.extend(extract_legal_markers(text))\n",
    "\n",
    "    # underthesea noun-ish phrases\n",
    "    ut = extract_terms_underthesea(text, top_k=top_k)\n",
    "    if ut:\n",
    "        terms.extend(ut)\n",
    "\n",
    "    # yake keyphrases\n",
    "    yk = extract_terms_yake(text, top_k=top_k)\n",
    "    if yk:\n",
    "        terms.extend(yk)\n",
    "\n",
    "    # fallback: common patterns like \"cơ quan thuế\", \"người nộp thuế\" by scanning\n",
    "    # pick 2-5 word grams around these trigger words\n",
    "    triggers = [\"thuế\", \"hồ sơ\", \"nghĩa vụ\", \"quy định\", \"cơ quan\", \"người\", \"thương mại\", \"nền tảng\", \"hoàn\"]\n",
    "    words = re.findall(r\"[A-Za-zÀ-ỹ0-9_]+\", text.lower())\n",
    "    for i,w in enumerate(words):\n",
    "        if w in triggers:\n",
    "            for n in (2,3,4):\n",
    "                gram = \" \".join(words[i:i+n])\n",
    "                gram = normalize_term(gram)\n",
    "                if len(gram) >= MIN_TERM_LEN:\n",
    "                    terms.append(gram)\n",
    "\n",
    "    # clean + unique\n",
    "    clean = []\n",
    "    seen = set()\n",
    "    for t in terms:\n",
    "        t = normalize_term(t)\n",
    "        if not t:\n",
    "            continue\n",
    "        if len(t) < MIN_TERM_LEN:\n",
    "            continue\n",
    "        if len(t.split()) > MAX_TERM_WORDS:\n",
    "            continue\n",
    "        key = t.lower()\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            clean.append(t)\n",
    "\n",
    "    return clean[:top_k]\n",
    "\n",
    "# ==============\n",
    "# Build KG (NO LLM): co-occurrence graph\n",
    "# ==============\n",
    "def build_kg_and_chunk_entities_no_llm(dataset: List[str]) -> Tuple[nx.DiGraph, List[Set[str]]]:\n",
    "    kg = nx.DiGraph()\n",
    "    chunk_entities: List[Set[str]] = []\n",
    "\n",
    "    for i, chunk in enumerate(dataset):\n",
    "        terms = extract_terms_offline(chunk, top_k=TOP_K_TERMS_PER_CHUNK)\n",
    "        ent_set = set(terms)\n",
    "        chunk_entities.append(ent_set)\n",
    "\n",
    "        for t in ent_set:\n",
    "            if not kg.has_node(t):\n",
    "                kg.add_node(t)\n",
    "\n",
    "        # co-occurrence edges (ordered terms)\n",
    "        ordered = terms\n",
    "        for a_idx, a in enumerate(ordered):\n",
    "            for b_idx in range(a_idx+1, min(len(ordered), a_idx+1+COOC_WINDOW)):\n",
    "                b = ordered[b_idx]\n",
    "                if a == b:\n",
    "                    continue\n",
    "                # directional edge with relation label\n",
    "                if kg.has_edge(a, b):\n",
    "                    kg[a][b][\"weight\"] = kg[a][b].get(\"weight\", 1) + 1\n",
    "                else:\n",
    "                    kg.add_edge(a, b, relation=\"co_occurs_with\", weight=1, source_chunk=i)\n",
    "\n",
    "        if (i+1) % 200 == 0:\n",
    "            logger.info(f\"KG progress {i+1}/{len(dataset)} | nodes={kg.number_of_nodes()} edges={kg.number_of_edges()}\")\n",
    "\n",
    "    return kg, chunk_entities\n",
    "\n",
    "# ====== RUN OFFLINE BUILD ======\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "logger.info(\"Load JSON -> passages\")\n",
    "doc = load_legal_json(JSON_PATH)\n",
    "passages = flatten_to_passages(doc)\n",
    "\n",
    "logger.info(\"Chunking\")\n",
    "dataset = make_chunks(passages)\n",
    "logger.info(f\"chunks = {len(dataset)}\")\n",
    "\n",
    "logger.info(\"Embedding + FAISS\")\n",
    "embeddings = embed_texts(dataset, batch_size=BATCH_EMBED)\n",
    "index = build_faiss(embeddings)\n",
    "logger.info(f\"FAISS ntotal = {index.ntotal}\")\n",
    "\n",
    "logger.info(\"Build KG + chunk_entities (NO LLM)\")\n",
    "kg, chunk_entities = build_kg_and_chunk_entities_no_llm(dataset)\n",
    "\n",
    "# ====== SAVE ARTIFACTS ======\n",
    "with open(os.path.join(CACHE_DIR, \"chunks.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset, f, ensure_ascii=False)\n",
    "\n",
    "faiss.write_index(index, os.path.join(CACHE_DIR, \"faiss.index\"))\n",
    "np.save(os.path.join(CACHE_DIR, \"embeddings.npy\"), embeddings)  # optional\n",
    "\n",
    "# nx.write_gpickle(kg, os.path.join(CACHE_DIR, \"kg.pkl\"))\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(os.path.join(CACHE_DIR, \"kg.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(kg, f)\n",
    "\n",
    "with open(os.path.join(CACHE_DIR, \"chunk_entities.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump([list(s) for s in chunk_entities], f, ensure_ascii=False)\n",
    "\n",
    "meta = {\n",
    "    \"created_at\": datetime.now().isoformat(),\n",
    "    \"embedding_model\": EMBED_MODEL,\n",
    "    \"num_chunks\": len(dataset),\n",
    "    \"kg_nodes\": kg.number_of_nodes(),\n",
    "    \"kg_edges\": kg.number_of_edges(),\n",
    "    \"build_mode\": \"NO_LLM\",\n",
    "}\n",
    "with open(os.path.join(CACHE_DIR, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "logger.info(f\"✅ DONE. Saved artifacts to: {CACHE_DIR}\")\n",
    "logger.info(f\"KG nodes={kg.number_of_nodes()} edges={kg.number_of_edges()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33936ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Chatbot mode ready. Type question, 'exit' to stop.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, json, re\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "\n",
    "CACHE_DIR = \"./tt80_artifacts_no_llm\"\n",
    "MODEL_ANSWER = \"moonshotai/kimi-k2-instruct-0905\"\n",
    "TOP_K = 6\n",
    "ALPHA = 0.85\n",
    "\n",
    "# --- load artifacts once ---\n",
    "if \"BOT_READY\" not in globals():\n",
    "    with open(os.path.join(CACHE_DIR, \"meta.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        META = json.load(f)\n",
    "\n",
    "    with open(os.path.join(CACHE_DIR, \"chunks.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        DATASET = json.load(f)\n",
    "\n",
    "    INDEX = faiss.read_index(os.path.join(CACHE_DIR, \"faiss.index\"))\n",
    "    # KG = nx.read_gpickle(os.path.join(CACHE_DIR, \"kg.pkl\"))\n",
    "    \n",
    "    import pickle\n",
    "\n",
    "    with open(os.path.join(CACHE_DIR, \"kg.pkl\"), \"rb\") as f:\n",
    "        KG = pickle.load(f)\n",
    "\n",
    "    \n",
    "\n",
    "    with open(os.path.join(CACHE_DIR, \"chunk_entities.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        CHUNK_ENTITIES = [set(x) for x in json.load(f)]\n",
    "\n",
    "    EMBEDDER = SentenceTransformer(META[\"embedding_model\"])\n",
    "    BOT_READY = True\n",
    "\n",
    "    print(\"✅ Artifacts loaded\")\n",
    "    print(\"chunks:\", len(DATASET), \"| KG nodes:\", KG.number_of_nodes(), \"| KG edges:\", KG.number_of_edges())\n",
    "\n",
    "# --- LLM client (ONLY for answering) ---\n",
    "if not os.getenv(\"NVAPI_KEY\"):\n",
    "    raise RuntimeError(\"Missing NVAPI_KEY env var.\")\n",
    "client = OpenAI(base_url=\"https://integrate.api.nvidia.com/v1\", api_key=os.getenv(\"NVAPI_KEY\"))\n",
    "\n",
    "def llm_answer(prompt: str, temperature=0.2, max_tokens=450) -> str:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=MODEL_ANSWER,\n",
    "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "        temperature=temperature,\n",
    "        top_p=0.9,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "# --- fast keyword/entity extraction without LLM ---\n",
    "STOP = set([\"là\", \"và\", \"của\", \"theo\", \"đối\", \"với\", \"như\", \"thế\", \"nào\", \"gì\", \"bao\", \"gồm\", \"quy\", \"định\", \"hướng\", \"dẫn\", \"không\", \"có\", \"một\", \"các\", \"được\"])\n",
    "\n",
    "def fast_terms(query: str) -> Set[str]:\n",
    "    q = query.lower()\n",
    "    # keep quoted phrases\n",
    "    quoted = re.findall(r\"“([^”]+)”|\\\"([^\\\"]+)\\\"\", query)\n",
    "    terms = set()\n",
    "    for a,b in quoted:\n",
    "        s = (a or b).strip().lower()\n",
    "        if s:\n",
    "            terms.add(s)\n",
    "    # split words\n",
    "    words = re.findall(r\"[a-zA-ZÀ-ỹ0-9_]+\", q)\n",
    "    for w in words:\n",
    "        if len(w) >= 3 and w not in STOP:\n",
    "            terms.add(w)\n",
    "    return terms\n",
    "\n",
    "def semantic_search(query: str, top_k: int):\n",
    "    q_vec = EMBEDDER.encode([f\"query: {query}\"], normalize_embeddings=True).astype(\"float32\")\n",
    "    D, I = INDEX.search(q_vec, top_k)\n",
    "    return [(int(I[0][j]), float(D[0][j])) for j in range(top_k)]\n",
    "\n",
    "def graph_scores(query_terms: Set[str]) -> np.ndarray:\n",
    "    scores = np.zeros(len(DATASET), dtype=float)\n",
    "    if not query_terms:\n",
    "        return scores\n",
    "    for i, ents in enumerate(CHUNK_ENTITIES):\n",
    "        # match by substring (legal terms)\n",
    "        hit = 0\n",
    "        for t in query_terms:\n",
    "            for e in ents:\n",
    "                if t in e.lower():\n",
    "                    hit += 1\n",
    "                    break\n",
    "        scores[i] = hit\n",
    "    if scores.max() > 0:\n",
    "        scores = scores / (scores.max() + 1e-12)\n",
    "    return scores\n",
    "\n",
    "def kg_facts_from_terms(query_terms: Set[str], limit=10) -> List[str]:\n",
    "    facts = []\n",
    "    # fuzzy: find nodes containing any term\n",
    "    for t in list(query_terms)[:8]:\n",
    "        cand = [n for n in KG.nodes() if t in str(n).lower()]\n",
    "        for node in cand[:2]:\n",
    "            for nb in list(KG.successors(node))[:3]:\n",
    "                rel = KG.get_edge_data(node, nb).get(\"relation\", \"related_to\")\n",
    "                facts.append(f\"{node} --{rel}--> {nb}\")\n",
    "                if len(facts) >= limit:\n",
    "                    return facts\n",
    "    return facts\n",
    "\n",
    "def retrieve_context(query: str, top_k=TOP_K, alpha=ALPHA):\n",
    "    sem = semantic_search(query, top_k=top_k*2)\n",
    "    terms = fast_terms(query)\n",
    "    gs = graph_scores(terms)\n",
    "\n",
    "    combined = []\n",
    "    for idx, sem_score in sem:\n",
    "        g = float(gs[idx])\n",
    "        combined.append((idx, alpha*sem_score + (1-alpha)*g))\n",
    "    combined.sort(key=lambda x: x[1], reverse=True)\n",
    "    idxs = [i for i,_ in combined[:top_k]]\n",
    "\n",
    "    ctx = \"\\n\\n\".join([DATASET[i] for i in idxs])\n",
    "    facts = kg_facts_from_terms(terms, limit=10)\n",
    "    if facts:\n",
    "        ctx += \"\\n\\nKG_FACTS:\\n\" + \"\\n\".join(facts)\n",
    "    return ctx, idxs\n",
    "\n",
    "def ask(query: str):\n",
    "    ctx, idxs = retrieve_context(query)\n",
    "    prompt = f\"\"\"\n",
    "Bạn là trợ lý hỏi đáp luật Việt Nam.\n",
    "\n",
    "sử dụng những kiến thức từ knowledge graph, không tự bịa đặt nhé\n",
    "\n",
    "hơn nữa, những kiến thức có trong CONTEXT rất quan trọng, phải dùng đến chúng để trả lời câu hỏi. Nên hãy dùng hết để tóm tắt để trả lời\n",
    "\n",
    "CHỈ trả lời dựa trên CONTEXT. Không bịa.\n",
    "Nếu CONTEXT không có thông tin, trả lời đúng 1 dòng:\n",
    "no relevant information found\n",
    "\n",
    "CONTEXT:\n",
    "{ctx}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "    ans = llm_answer(prompt, temperature=0.2, max_tokens=450)\n",
    "    return ans, idxs\n",
    "\n",
    "print(\"\\n✅ Chatbot mode ready. Type question, 'exit' to stop.\\n\")\n",
    "# while True:\n",
    "#     q = input(\"You: \").strip()\n",
    "#     if not q:\n",
    "#         continue\n",
    "#     if q.lower() in (\"exit\",\"quit\",\"q\"):\n",
    "#         print(\"Bye!\")\n",
    "#         break\n",
    "#     ans, idxs = ask(q)\n",
    "#     print(f\"\\nBot: {ans}\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d536e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "Hoạt động thương mại điện tử được hiểu như thế nào?\n",
      "\n",
      "CONTEXT USED:\n",
      "[clause] Văn bản: Thông tư 80/2021/TT-BTC hướng dẫn Luật Quản lý thuế  Nghị định 126/2020/NĐ-CP > URL: https://thuvienphapluat.vn/van-ban/Thue-Phi-Le-Phi/Thong-tu-80-2021-TT-BTC-huong-dan-Luat-Quan-ly-thue-Nghi-dinh-126-2020-ND-CP-466716.aspx > Crawled: 2026-01-24 > Chương I\n",
      "NHỮNG QUY ĐỊNH CHUNG > Điều 2. Đối tượng áp dụng Đối tượng áp dụng Thông tư này bao gồm: Người nộp thuế; cơ quan thuế; công chức thuế; cơ quan nhà nước, tổ chức, cá nhân khác có liên quan theo quy định tại Điều 2 Luật Quản lý thuế. Điều 3. Giải thích từ ngữ > 1\n",
      "“Hoạt động\n",
      "thương mại điện tử” là việc tiến hành một phần hoặc toàn bộ quy trình của hoạt\n",
      "động thương mại bằng phương tiện điện tử có kết nối với mạng Internet, mạng viễn\n",
      "thông di động hoặc các mạng mở khác theo quy định tại Nghị định số 52/2013/NĐ-CP ngày 16/5/2013 của Chính phủ.\n",
      "\n",
      "[clause] Văn bản: Thông tư 80/2021/TT-BTC hướng dẫn Luật Quản lý thuế  Nghị định 126/2020/NĐ-CP > URL: https://thuvienphapluat.vn/van-ban/Thue-Phi-Le-Phi/Thong-tu-80-2021-TT-BTC-huong-dan-Luat-Quan-ly-thue-Nghi-dinh-126-2020-ND-CP-466716.aspx > Crawled: 2026-01-24 > Chương IX\n",
      "QUẢN LÝ THUẾ ĐỐI VỚI HOẠT\n",
      "ĐỘNG KINH DOANH THƯƠNG MẠI ĐIỆN TỬ, KINH DOANH DỰA TRÊN NỀN TẢNG SỐ VÀ CÁC DỊCH\n",
      "VỤ KHÁC CỦA NHÀ CUNG CẤP Ở NƯỚC NGOÀI KHÔNG CÓ CƠ SỞ THƯỜNG TRÚ TẠI VIỆT NAM > Điều 73. Các tổ chức, cá nhân có liên quan trong việc quản lý thuế đối với hoạt động kinh doanh thương mại điện tử, kinh doanh dựa trên nền tảng số và các dịch vụ khác của nhà cung cấp ở nước ngoài không có cơ sở thường trú tại Việt Nam > 3\n",
      "Tổ chức, đại lý thuế hoạt động theo pháp luật Việt Nam được nhà cung cấp ở nước\n",
      "ngoài ủy quyền thực hiện đăng ký thuế, khai thuế, nộp thuế tại Việt Nam.\n",
      "\n",
      "[clause] Văn bản: Thông tư 80/2021/TT-BTC hướng dẫn Luật Quản lý thuế  Nghị định 126/2020/NĐ-CP > URL: https://thuvienphapluat.vn/van-ban/Thue-Phi-Le-Phi/Thong-tu-80-2021-TT-BTC-huong-dan-Luat-Quan-ly-thue-Nghi-dinh-126-2020-ND-CP-466716.aspx > Crawled: 2026-01-24 > Chương IX\n",
      "QUẢN LÝ THUẾ ĐỐI VỚI HOẠT\n",
      "ĐỘNG KINH DOANH THƯƠNG MẠI ĐIỆN TỬ, KINH DOANH DỰA TRÊN NỀN TẢNG SỐ VÀ CÁC DỊCH\n",
      "VỤ KHÁC CỦA NHÀ CUNG CẤP Ở NƯỚC NGOÀI KHÔNG CÓ CƠ SỞ THƯỜNG TRÚ TẠI VIỆT NAM > Điều 80. Trách nhiệm của cơ quan thuế trong việc quản lý thuế đối với hoạt động kinh doanh thương mại điện tử, kinh doanh dựa trên nền tảng số và các dịch vụ khác được thực hiện bởi nhà cung cấp ở nước ngoài > 2\n",
      "Cập nhật danh sách các nhà cung cấp nước nước ngoài trực tiếp hoặc ủy quyền đăng ký thuế,\n",
      "kê khai thuế trên Cổng thông tin điện tử của Tổng cục Thuế.\n",
      "\n",
      "[clause] Văn bản: Thông tư 80/2021/TT-BTC hướng dẫn Luật Quản lý thuế  Nghị định 126/2020/NĐ-CP > URL: https://thuvienphapluat.vn/van-ban/Thue-Phi-Le-Phi/Thong-tu-80-2021-TT-BTC-huong-dan-Luat-Quan-ly-thue-Nghi-dinh-126-2020-ND-CP-466716.aspx > Crawled: 2026-01-24 > Chương IX\n",
      "QUẢN LÝ THUẾ ĐỐI VỚI HOẠT\n",
      "ĐỘNG KINH DOANH THƯƠNG MẠI ĐIỆN TỬ, KINH DOANH DỰA TRÊN NỀN TẢNG SỐ VÀ CÁC DỊCH\n",
      "VỤ KHÁC CỦA NHÀ CUNG CẤP Ở NƯỚC NGOÀI KHÔNG CÓ CƠ SỞ THƯỜNG TRÚ TẠI VIỆT NAM > Điều 80. Trách nhiệm của cơ quan thuế trong việc quản lý thuế đối với hoạt động kinh doanh thương mại điện tử, kinh doanh dựa trên nền tảng số và các dịch vụ khác được thực hiện bởi nhà cung cấp ở nước ngoài > 3\n",
      "Phối hợp với cơ\n",
      "quan có liên quan xác định, công bố tên, địa chỉ website của nhà cung cấp ở nước\n",
      "ngoài chưa thực hiện đăng ký, kê khai, nộp thuế mà người mua hàng hóa, dịch vụ\n",
      "có thực hiện giao dịch\n",
      "phát sinh tại Việt Nam.\n",
      "\n",
      "[clause] Văn bản: Thông tư 80/2021/TT-BTC hướng dẫn Luật Quản lý thuế  Nghị định 126/2020/NĐ-CP > URL: https://thuvienphapluat.vn/van-ban/Thue-Phi-Le-Phi/Thong-tu-80-2021-TT-BTC-huong-dan-Luat-Quan-ly-thue-Nghi-dinh-126-2020-ND-CP-466716.aspx > Crawled: 2026-01-24 > Chương IX\n",
      "QUẢN LÝ THUẾ ĐỐI VỚI HOẠT\n",
      "ĐỘNG KINH DOANH THƯƠNG MẠI ĐIỆN TỬ, KINH DOANH DỰA TRÊN NỀN TẢNG SỐ VÀ CÁC DỊCH\n",
      "VỤ KHÁC CỦA NHÀ CUNG CẤP Ở NƯỚC NGOÀI KHÔNG CÓ CƠ SỞ THƯỜNG TRÚ TẠI VIỆT NAM > Điều 73. Các tổ chức, cá nhân có liên quan trong việc quản lý thuế đối với hoạt động kinh doanh thương mại điện tử, kinh doanh dựa trên nền tảng số và các dịch vụ khác của nhà cung cấp ở nước ngoài không có cơ sở thường trú tại Việt Nam > 2\n",
      "Tổ chức, cá nhân ở Việt Nam mua hàng hóa, dịch vụ của nhà cung cấp ở nước\n",
      "ngoài.\n",
      "\n",
      "[clause] Văn bản: Thông tư 80/2021/TT-BTC hướng dẫn Luật Quản lý thuế  Nghị định 126/2020/NĐ-CP > URL: https://thuvienphapluat.vn/van-ban/Thue-Phi-Le-Phi/Thong-tu-80-2021-TT-BTC-huong-dan-Luat-Quan-ly-thue-Nghi-dinh-126-2020-ND-CP-466716.aspx > Crawled: 2026-01-24 > Chương IX\n",
      "QUẢN LÝ THUẾ ĐỐI VỚI HOẠT\n",
      "ĐỘNG KINH DOANH THƯƠNG MẠI ĐIỆN TỬ, KINH DOANH DỰA TRÊN NỀN TẢNG SỐ VÀ CÁC DỊCH\n",
      "VỤ KHÁC CỦA NHÀ CUNG CẤP Ở NƯỚC NGOÀI KHÔNG CÓ CƠ SỞ THƯỜNG TRÚ TẠI VIỆT NAM > Điều 73. Các tổ chức, cá nhân có liên quan trong việc quản lý thuế đối với hoạt động kinh doanh thương mại điện tử, kinh doanh dựa trên nền tảng số và các dịch vụ khác của nhà cung cấp ở nước ngoài không có cơ sở thường trú tại Việt Nam > 4\n",
      "Ngân hàng thương\n",
      "mại, tổ chức cung ứng dịch vụ trung gian thanh toán và các tổ chức, cá\n",
      "nhân có\n",
      "quyền và nghĩa vụ liên quan đến hoạt động kinh doanh thương mại điện tử, kinh\n",
      "doanh dựa trên nền tảng số và các dịch vụ khác của nhà cung cấp ở nước\n",
      "ngoài.\n",
      "\n",
      "KG_FACTS:\n",
      "Hoạt động thương mại điện tử --co_occurs_with--> Điều 2\n",
      "Hoạt động thương mại điện tử --co_occurs_with--> Điều 3\n",
      "thuế hoạt động --co_occurs_with--> thuế hoạt động theo\n",
      "thuế hoạt động --co_occurs_with--> thuế chịu\n",
      "thuế hoạt động --co_occurs_with--> thuế khai\n",
      "Hoạt động thương mại điện tử --co_occurs_with--> Điều 2\n",
      "Hoạt động thương mại điện tử --co_occurs_with--> Điều 3\n",
      "Hoạt động thương mại điện tử --co_occurs_with--> Điều 2\n",
      "Hoạt động thương mại điện tử --co_occurs_with--> Điều 3\n",
      "Hoạt động thương mại điện tử --co_occurs_with--> Điều 2\n",
      "\n",
      "ANSWER:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://integrate.api.nvidia.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Hoạt động thương mại điện tử” là việc tiến hành một phần hoặc toàn bộ quy trình của hoạt động thương mại bằng phương tiện điện tử có kết nối với mạng Internet, mạng viễn thông di động hoặc các mạng mở khác theo quy định tại Nghị định số 52/2013/NĐ-CP ngày 16/5/2013 của Chính phủ.\n",
      "\n",
      "USED CHUNKS:\n",
      "[3, 1166, 1205, 1206, 1165, 1167]\n"
     ]
    }
   ],
   "source": [
    "question = \"Hoạt động thương mại điện tử được hiểu như thế nào?\"\n",
    "\n",
    "context, used_chunks = retrieve_context(question)\n",
    "\n",
    "print(\"QUESTION:\")\n",
    "print(question)\n",
    "\n",
    "print(\"\\nCONTEXT USED:\")\n",
    "print(context)\n",
    "\n",
    "print(\"\\nANSWER:\")\n",
    "print(llm_answer(f\"\"\"\n",
    "Bạn là trợ lý hỏi đáp luật Việt Nam.\n",
    "\n",
    "CHỈ trả lời dựa trên CONTEXT.\n",
    "Nếu không có thông tin, trả lời:\n",
    "no relevant information found\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"))\n",
    "\n",
    "print(\"\\nUSED CHUNKS:\")\n",
    "print(used_chunks)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-graphrag (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
