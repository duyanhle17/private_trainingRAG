{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "081f1c62",
   "metadata": {},
   "source": [
    "SET UP m√¥i tr∆∞·ªùng nh√© h·∫π h·∫π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "332cf19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (5.2.2)\n",
      "Requirement already satisfied: faiss-cpu in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: pandas in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from sentence-transformers) (5.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from sentence-transformers) (1.3.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from sentence-transformers) (2.10.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from sentence-transformers) (1.8.0)\n",
      "Requirement already satisfied: scipy in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from sentence-transformers) (1.17.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: tqdm in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (3.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2026.1.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.5.4)\n",
      "Requirement already satisfied: anyio in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.10.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from typer-slim->transformers<6.0.0,>=4.41.0->sentence-transformers) (8.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers faiss-cpu networkx pandas numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c30f35fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install tiktoken\n",
    "\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e401c08f",
   "metadata": {},
   "source": [
    "LLM extractor (JSON schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "775971a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# set key ·ªü ƒë√¢y (ƒë·ª´ng hardcode trong code d√†i)\n",
    "os.environ[\"NVAPI_KEY\"] = \"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\"  # <-- ƒëi·ªÅn key c·ªßa b·∫°n\n",
    "\n",
    "JSON_PATH = \"thongtu80.json\"\n",
    "CACHE_DIR = \"./tt80_cache_full\"   # th∆∞ m·ª•c l∆∞u m·ªçi th·ª©\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45a015fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (2.16.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from openai) (2.12.5)\n",
      "Requirement already satisfied: sniffio in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: colorama in c:\\duy anh le code\\private_trainingrag\\.venv-graphrag\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e369d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: intfloat/multilingual-e5-base\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/multilingual-e5-base/835193815a3936a24a0ee7dc9e3d48c1fbb19c55/modules.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/config_sentence_transformers.json \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/config_sentence_transformers.json \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/multilingual-e5-base/835193815a3936a24a0ee7dc9e3d48c1fbb19c55/README.md \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/multilingual-e5-base/835193815a3936a24a0ee7dc9e3d48c1fbb19c55/modules.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/sentence_bert_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/multilingual-e5-base/835193815a3936a24a0ee7dc9e3d48c1fbb19c55/sentence_bert_config.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/adapter_config.json \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/multilingual-e5-base/835193815a3936a24a0ee7dc9e3d48c1fbb19c55/config.json \"HTTP/1.1 200 OK\"\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199/199 [00:00<00:00, 982.10it/s, Materializing param=pooler.dense.weight]                               \n",
      "XLMRobertaModel LOAD REPORT from: intfloat/multilingual-e5-base\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/multilingual-e5-base/835193815a3936a24a0ee7dc9e3d48c1fbb19c55/config.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/multilingual-e5-base/835193815a3936a24a0ee7dc9e3d48c1fbb19c55/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/intfloat/multilingual-e5-base/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/intfloat/multilingual-e5-base/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/intfloat/multilingual-e5-base \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/1_Pooling/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/multilingual-e5-base/835193815a3936a24a0ee7dc9e3d48c1fbb19c55/1_Pooling%2Fconfig.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/intfloat/multilingual-e5-base \"HTTP/1.1 200 OK\"\n",
      "INFO:OFFLINE_BUILD_NO_LLM:============================================================\n",
      "INFO:OFFLINE_BUILD_NO_LLM:LOADING MULTIPLE JSON FILES\n",
      "INFO:OFFLINE_BUILD_NO_LLM:============================================================\n",
      "INFO:OFFLINE_BUILD_NO_LLM:üìÑ Loading: thongtu80.json\n",
      "INFO:OFFLINE_BUILD_NO_LLM:   ‚Üí 1244 passages\n",
      "INFO:OFFLINE_BUILD_NO_LLM:üìÑ Loading: thongtu78.json\n",
      "INFO:OFFLINE_BUILD_NO_LLM:   ‚Üí 110 passages\n",
      "INFO:OFFLINE_BUILD_NO_LLM:üìÑ Loading: nghidinh123.json\n",
      "INFO:OFFLINE_BUILD_NO_LLM:   ‚Üí 493 passages\n",
      "INFO:OFFLINE_BUILD_NO_LLM:üìÑ Loading: nghidinh125.json\n",
      "INFO:OFFLINE_BUILD_NO_LLM:   ‚Üí 432 passages\n",
      "INFO:OFFLINE_BUILD_NO_LLM:üìÑ Loading: GTGT2008.json\n",
      "INFO:OFFLINE_BUILD_NO_LLM:   ‚Üí 97 passages\n",
      "INFO:OFFLINE_BUILD_NO_LLM:üìÑ Loading: QLT2019.json\n",
      "INFO:OFFLINE_BUILD_NO_LLM:   ‚Üí 1060 passages\n",
      "INFO:OFFLINE_BUILD_NO_LLM:‚úÖ Total: 3436 passages from 6 files\n",
      "INFO:OFFLINE_BUILD_NO_LLM:\n",
      "Chunking\n",
      "INFO:OFFLINE_BUILD_NO_LLM:Chunking mode: tiktoken\n",
      "INFO:OFFLINE_BUILD_NO_LLM:Total chunks = 3488\n",
      "INFO:OFFLINE_BUILD_NO_LLM:\n",
      "Embedding + FAISS\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.98s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.41s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.27s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.92s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.76s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.31s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.54s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.71s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.49s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.65s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.58s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.82s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.30s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.05s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.64s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.50s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.55s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.50s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.96s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.32s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.82s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.87s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.82s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.94s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.15s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.60s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.27s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.50s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.92s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.14s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.81s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.49s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.29s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.22s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.79s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.01s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.91s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.01s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.78s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.14s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.76s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.35s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.03s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.67s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.86s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.60s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.02s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.66s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.25s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.12s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.65s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.39s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.74s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.85s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.80s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.61s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.02s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.49s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.57s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.04s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.83s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.24s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.70s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.62s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.96s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.72s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.89s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.48s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.71s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.71s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.17s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.70s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.89s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.15s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.87s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.84s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.88s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.47s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.96s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.41s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.37s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.99s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.46s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.45s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.33s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.26s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.73s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.15s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.44s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.91s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.40s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.82s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.01s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.35s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.77s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.11s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.83s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.23s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.66s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.14s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.24s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.68s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.74s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.35s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.84s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.62s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.15s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.45s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.58s/it]\n",
      "INFO:OFFLINE_BUILD_NO_LLM:FAISS ntotal = 3488\n",
      "INFO:OFFLINE_BUILD_NO_LLM:\n",
      "Build KG + chunk_entities (NO LLM)\n",
      "INFO:OFFLINE_BUILD_NO_LLM:KG progress 200/3488 | nodes=441 edges=1227\n",
      "INFO:OFFLINE_BUILD_NO_LLM:KG progress 400/3488 | nodes=673 edges=2100\n",
      "INFO:OFFLINE_BUILD_NO_LLM:KG progress 600/3488 | nodes=782 edges=2563\n",
      "INFO:OFFLINE_BUILD_NO_LLM:KG progress 800/3488 | nodes=807 edges=2681\n",
      "INFO:OFFLINE_BUILD_NO_LLM:KG progress 1000/3488 | nodes=835 edges=2779\n",
      "INFO:OFFLINE_BUILD_NO_LLM:KG progress 1200/3488 | nodes=1118 edges=3843\n",
      "INFO:OFFLINE_BUILD_NO_LLM:KG progress 1400/3488 | nodes=1558 edges=5487\n",
      "INFO:OFFLINE_BUILD_NO_LLM:KG progress 1600/3488 | nodes=1915 edges=6840\n",
      "INFO:OFFLINE_BUILD_NO_LLM:KG progress 1800/3488 | nodes=2253 edges=8225\n",
      "INFO:OFFLINE_BUILD_NO_LLM:KG progress 2000/3488 | nodes=2469 edges=9171\n",
      "INFO:OFFLINE_BUILD_NO_LLM:KG progress 2200/3488 | nodes=2670 edges=10067\n",
      "INFO:OFFLINE_BUILD_NO_LLM:KG progress 2400/3488 | nodes=2996 edges=11197\n",
      "INFO:OFFLINE_BUILD_NO_LLM:KG progress 2600/3488 | nodes=3398 edges=12832\n",
      "INFO:OFFLINE_BUILD_NO_LLM:KG progress 2800/3488 | nodes=3781 edges=14538\n",
      "INFO:OFFLINE_BUILD_NO_LLM:KG progress 3000/3488 | nodes=4061 edges=15970\n",
      "INFO:OFFLINE_BUILD_NO_LLM:KG progress 3200/3488 | nodes=4310 edges=17047\n",
      "INFO:OFFLINE_BUILD_NO_LLM:KG progress 3400/3488 | nodes=4596 edges=18146\n",
      "INFO:OFFLINE_BUILD_NO_LLM:\n",
      "============================================================\n",
      "INFO:OFFLINE_BUILD_NO_LLM:‚úÖ DONE. Saved artifacts to: ./tt80_artifacts_no_llm\n",
      "INFO:OFFLINE_BUILD_NO_LLM:   Files processed: 6\n",
      "INFO:OFFLINE_BUILD_NO_LLM:   Total chunks: 3488\n",
      "INFO:OFFLINE_BUILD_NO_LLM:   KG nodes: 4724\n",
      "INFO:OFFLINE_BUILD_NO_LLM:   KG edges: 18778\n",
      "INFO:OFFLINE_BUILD_NO_LLM:============================================================\n"
     ]
    }
   ],
   "source": [
    "import os, json, re, logging\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Optional, Set, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"OFFLINE_BUILD_NO_LLM\")\n",
    "\n",
    "# config\n",
    "# JSON_PATH  = \"thongtu80.json\"\n",
    "JSON_FILES = [\n",
    "    \"thongtu80.json\",\n",
    "    \"thongtu78.json\",\n",
    "    \"nghidinh123.json\",\n",
    "    \"nghidinh125.json\",\n",
    "    \"GTGT2008.json\",\n",
    "    \"QLT2019.json\"\n",
    "]\n",
    "\n",
    "CACHE_DIR  = \"./tt80_artifacts_no_llm\"\n",
    "\n",
    "EMBED_MODEL = \"intfloat/multilingual-e5-base\"\n",
    "BATCH_EMBED = 32\n",
    "\n",
    "CHUNK_TOKENS = 800\n",
    "CHUNK_OVERLAP = 80\n",
    "CHUNK_CHAR = 1200\n",
    "CHUNK_CHAR_OVERLAP = 120\n",
    "\n",
    "# KG params (offline)\n",
    "TOP_K_TERMS_PER_CHUNK = 18\n",
    "MIN_TERM_LEN = 3\n",
    "MAX_TERM_WORDS = 6\n",
    "COOC_WINDOW = 2  # connect term i with i+1..i+COOC_WINDOW in the same chunk\n",
    "\n",
    "# ph·∫ßn n√†y l√† ƒë·ªÉ x·ª≠ l√Ω JSON vƒÉn b·∫£n ph√°p lu·∫≠t\n",
    "def norm(s: Optional[str]) -> str:\n",
    "    return (s or \"\").replace(\"\\r\", \"\").strip()\n",
    "\n",
    "def join_path(parts: List[str]) -> str:\n",
    "    return \" > \".join([p for p in parts if p])\n",
    "\n",
    "def flatten_legal_tree(node: Dict[str, Any], path: List[str], out: List[Dict[str, Any]]):\n",
    "    ntype = node.get(\"type\", \"\")\n",
    "    title = norm(node.get(\"title\"))\n",
    "    content = norm(node.get(\"content\"))\n",
    "    new_path = path + ([title] if title else [])\n",
    "    if content:\n",
    "        out.append({\"type\": ntype, \"path\": join_path(new_path), \"text\": content})\n",
    "    for ch in node.get(\"children\", []) or []:\n",
    "        flatten_legal_tree(ch, new_path, out)\n",
    "\n",
    "def load_legal_json(json_path: str) -> Dict[str, Any]:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def flatten_to_passages(doc: Dict[str, Any], source_file: str = \"\") -> List[str]:\n",
    "    \"\"\"Flatten 1 document th√†nh passages, th√™m source_file v√†o metadata\"\"\"\n",
    "    info = doc.get(\"document_info\", {})\n",
    "    base = [\n",
    "        f\"VƒÉn b·∫£n: {norm(info.get('title'))}\",\n",
    "    ]\n",
    "    if source_file:\n",
    "        base.append(f\"Ngu·ªìn: {source_file}\")\n",
    "    base.extend([\n",
    "        f\"URL: {norm(info.get('url'))}\",\n",
    "        f\"Crawled: {norm(info.get('crawled_at'))}\",\n",
    "    ])\n",
    "    \n",
    "    units: List[Dict[str, Any]] = []\n",
    "    for top in doc.get(\"body\", []) or []:\n",
    "        flatten_legal_tree(top, base, units)\n",
    "\n",
    "    passages = []\n",
    "    for u in units:\n",
    "        header = f\"[{u['type']}] {u['path']}\"\n",
    "        passages.append(header + \"\\n\" + u[\"text\"])\n",
    "    return passages\n",
    "\n",
    "# ============================================================\n",
    "# LOAD MULTIPLE JSON FILES\n",
    "# ============================================================\n",
    "\n",
    "def load_all_json_files(json_files: List[str]) -> Tuple[List[str], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load t·∫•t c·∫£ JSON files v√† tr·∫£ v·ªÅ:\n",
    "    - all_passages: list t·∫•t c·∫£ passages t·ª´ m·ªçi file\n",
    "    - metadata: th√¥ng tin v·ªÅ s·ªë l∆∞·ª£ng passages t·ª´ m·ªói file\n",
    "    \"\"\"\n",
    "    all_passages = []\n",
    "    metadata = {\n",
    "        \"files\": [],\n",
    "        \"passages_per_file\": {}\n",
    "    }\n",
    "    \n",
    "    for json_path in json_files:\n",
    "        if not os.path.exists(json_path):\n",
    "            logger.warning(f\"‚ö†Ô∏è File not found: {json_path}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        logger.info(f\"üìÑ Loading: {json_path}\")\n",
    "        try:\n",
    "            doc = load_legal_json(json_path)\n",
    "            passages = flatten_to_passages(doc, source_file=json_path)\n",
    "            \n",
    "            metadata[\"files\"].append(json_path)\n",
    "            metadata[\"passages_per_file\"][json_path] = len(passages)\n",
    "            \n",
    "            all_passages.extend(passages)\n",
    "            logger.info(f\"   ‚Üí {len(passages)} passages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error loading {json_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    logger.info(f\"‚úÖ Total: {len(all_passages)} passages from {len(metadata['files'])} files\")\n",
    "    return all_passages, metadata\n",
    "\n",
    "\n",
    "# Chunking nh√© ae\n",
    "def try_import_tiktoken():\n",
    "    try:\n",
    "        import tiktoken\n",
    "        return tiktoken\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def token_chunk(text: str, chunk_tokens: int, overlap: int, tokenizer_name=\"cl100k_base\") -> List[str]:\n",
    "    tiktoken = try_import_tiktoken()\n",
    "    if tiktoken is None:\n",
    "        raise ModuleNotFoundError(\"tiktoken not installed\")\n",
    "    enc = tiktoken.get_encoding(tokenizer_name)\n",
    "    toks = enc.encode(text)\n",
    "    step = max(1, chunk_tokens - overlap)\n",
    "    out = []\n",
    "    for i in range(0, len(toks), step):\n",
    "        sub = toks[i:i+chunk_tokens]\n",
    "        if sub:\n",
    "            out.append(enc.decode(sub))\n",
    "    return out\n",
    "\n",
    "def char_chunk(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
    "    seps = [\"\\n\\n\", \"\\n\", \". \", \"; \", \", \", \" \", \"\"]\n",
    "    chunks: List[str] = []\n",
    "    buf = text.strip()\n",
    "    if not buf:\n",
    "        return []\n",
    "    start = 0\n",
    "    while start < len(buf):\n",
    "        end = min(len(buf), start + chunk_size)\n",
    "        piece = buf[start:end]\n",
    "        cut = -1\n",
    "        for sep in seps:\n",
    "            idx = piece.rfind(sep)\n",
    "            if idx > cut:\n",
    "                cut = idx\n",
    "        if cut <= 0:\n",
    "            cut = len(piece)\n",
    "        else:\n",
    "            cut = cut + 1\n",
    "        chunk = piece[:cut].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        start = start + max(1, cut - overlap)\n",
    "    return chunks\n",
    "\n",
    "def make_chunks(passages: List[str]) -> List[str]:\n",
    "    dataset: List[str] = []\n",
    "    use_token = try_import_tiktoken() is not None\n",
    "    logger.info(f\"Chunking mode: {'tiktoken' if use_token else 'char-fallback'}\")\n",
    "    for p in passages:\n",
    "        if use_token:\n",
    "            try:\n",
    "                dataset.extend(token_chunk(p, CHUNK_TOKENS, CHUNK_OVERLAP))\n",
    "            except Exception:\n",
    "                dataset.extend(char_chunk(p, CHUNK_CHAR, CHUNK_CHAR_OVERLAP))\n",
    "        else:\n",
    "            dataset.extend(char_chunk(p, CHUNK_CHAR, CHUNK_CHAR_OVERLAP))\n",
    "    return dataset\n",
    "\n",
    "# ==============\n",
    "# Embedding + FAISS\n",
    "# ==============\n",
    "embedder = SentenceTransformer(EMBED_MODEL)\n",
    "\n",
    "def embed_texts(texts: List[str], batch_size=32) -> np.ndarray:\n",
    "    vecs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inp = [f\"passage: {t}\" for t in batch]\n",
    "        v = embedder.encode(inp, normalize_embeddings=True)\n",
    "        vecs.append(np.asarray(v, dtype=\"float32\"))\n",
    "    return np.vstack(vecs)\n",
    "\n",
    "def build_faiss(embeddings: np.ndarray) -> faiss.Index:\n",
    "    d = embeddings.shape[1]\n",
    "    idx = faiss.IndexFlatIP(d)\n",
    "    idx.add(embeddings.astype(\"float32\"))\n",
    "    return idx\n",
    "\n",
    "# ==============\n",
    "# OFFLINE Entity / Term extraction (NO LLM)\n",
    "# ==============\n",
    "def normalize_term(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    # remove surrounding quotes\n",
    "    s = s.strip(\"‚Äú‚Äù\\\"' \")\n",
    "    return s\n",
    "\n",
    "def extract_quoted_terms(text: str) -> List[str]:\n",
    "    # captures ‚Äú...‚Äù, \"...\", '...'\n",
    "    terms = []\n",
    "    for a,b,c in re.findall(r\"‚Äú([^‚Äù]{2,120})‚Äù|\\\"([^\\\"]{2,120})\\\"|'([^']{2,120})'\", text):\n",
    "        t = normalize_term(a or b or c)\n",
    "        if t:\n",
    "            terms.append(t)\n",
    "    return terms\n",
    "\n",
    "def extract_legal_markers(text: str) -> List[str]:\n",
    "    # keep lawy concepts + headings\n",
    "    # examples: ƒêi·ªÅu 1, Kho·∫£n 2, ƒëi·ªÉm a, Ngh·ªã ƒë·ªãnh s·ªë 126/2020/Nƒê-CP\n",
    "    pats = [\n",
    "        r\"(ƒêi·ªÅu\\s+\\d+[A-Za-z]?)\",\n",
    "        r\"(Kho·∫£n\\s+\\d+[A-Za-z]?)\",\n",
    "        r\"(ƒëi·ªÉm\\s+[a-zƒë])\",\n",
    "        r\"(Ngh·ªã\\s+ƒë·ªãnh\\s+s·ªë\\s+\\d+\\/\\d+\\/[A-Zƒê\\-]+)\",\n",
    "        r\"(Th√¥ng\\s+t∆∞\\s+\\d+\\/\\d+\\/[A-Zƒê\\-]+)\",\n",
    "        r\"(Lu·∫≠t\\s+[A-Z√Ä-·ª∏a-z√†-·ªπ\\s]{3,80})\",\n",
    "    ]\n",
    "    out = []\n",
    "    for pat in pats:\n",
    "        out.extend([normalize_term(x) for x in re.findall(pat, text, flags=re.I)])\n",
    "    return out\n",
    "\n",
    "def extract_terms_yake(text: str, top_k: int) -> List[str]:\n",
    "    try:\n",
    "        import yake\n",
    "        kw_extractor = yake.KeywordExtractor(lan=\"vi\", n=MAX_TERM_WORDS, top=top_k)\n",
    "        kws = kw_extractor.extract_keywords(text)\n",
    "        # yake returns list[(kw, score)] lower score better\n",
    "        kws = sorted(kws, key=lambda x: x[1])[:top_k]\n",
    "        return [normalize_term(k) for k,_ in kws if k]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def extract_terms_underthesea(text: str, top_k: int) -> List[str]:\n",
    "    # optional: better Vietnamese word segmentation + noun phrases\n",
    "    try:\n",
    "        from underthesea import pos_tag\n",
    "        tags = pos_tag(text)\n",
    "        # collect sequences of N / NP / A (rough)\n",
    "        terms = []\n",
    "        buf = []\n",
    "        for w,t in tags:\n",
    "            if t in (\"N\", \"Np\", \"Ny\", \"Nc\", \"A\"):\n",
    "                buf.append(w)\n",
    "            else:\n",
    "                if 1 <= len(buf) <= MAX_TERM_WORDS:\n",
    "                    terms.append(normalize_term(\" \".join(buf)))\n",
    "                buf = []\n",
    "        if 1 <= len(buf) <= MAX_TERM_WORDS:\n",
    "            terms.append(normalize_term(\" \".join(buf)))\n",
    "        # filter length\n",
    "        terms = [x for x in terms if len(x) >= MIN_TERM_LEN]\n",
    "        # unique preserve order\n",
    "        seen = set()\n",
    "        out = []\n",
    "        for x in terms:\n",
    "            lx = x.lower()\n",
    "            if lx not in seen:\n",
    "                seen.add(lx)\n",
    "                out.append(x)\n",
    "        return out[:top_k]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def extract_terms_offline(text: str, top_k: int = TOP_K_TERMS_PER_CHUNK) -> List[str]:\n",
    "    # Priority order: quoted terms + legal markers + underthesea + yake + fallback regex ngrams\n",
    "    terms = []\n",
    "    terms.extend(extract_quoted_terms(text))\n",
    "    terms.extend(extract_legal_markers(text))\n",
    "\n",
    "    # underthesea noun-ish phrases\n",
    "    ut = extract_terms_underthesea(text, top_k=top_k)\n",
    "    if ut:\n",
    "        terms.extend(ut)\n",
    "\n",
    "    # yake keyphrases\n",
    "    yk = extract_terms_yake(text, top_k=top_k)\n",
    "    if yk:\n",
    "        terms.extend(yk)\n",
    "\n",
    "    # fallback: common patterns like \"c∆° quan thu·∫ø\", \"ng∆∞·ªùi n·ªôp thu·∫ø\" by scanning\n",
    "    # pick 2-5 word grams around these trigger words\n",
    "    triggers = [\"thu·∫ø\", \"h·ªì s∆°\", \"nghƒ©a v·ª•\", \"quy ƒë·ªãnh\", \"c∆° quan\", \"ng∆∞·ªùi\", \"th∆∞∆°ng m·∫°i\", \"n·ªÅn t·∫£ng\", \"ho√†n\"]\n",
    "    words = re.findall(r\"[A-Za-z√Ä-·ªπ0-9_]+\", text.lower())\n",
    "    for i,w in enumerate(words):\n",
    "        if w in triggers:\n",
    "            for n in (2,3,4):\n",
    "                gram = \" \".join(words[i:i+n])\n",
    "                gram = normalize_term(gram)\n",
    "                if len(gram) >= MIN_TERM_LEN:\n",
    "                    terms.append(gram)\n",
    "\n",
    "    # clean + unique\n",
    "    clean = []\n",
    "    seen = set()\n",
    "    for t in terms:\n",
    "        t = normalize_term(t)\n",
    "        if not t:\n",
    "            continue\n",
    "        if len(t) < MIN_TERM_LEN:\n",
    "            continue\n",
    "        if len(t.split()) > MAX_TERM_WORDS:\n",
    "            continue\n",
    "        key = t.lower()\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            clean.append(t)\n",
    "\n",
    "    return clean[:top_k]\n",
    "\n",
    "# ==============\n",
    "# Build KG (NO LLM): co-occurrence graph\n",
    "# ==============\n",
    "def build_kg_and_chunk_entities_no_llm(dataset: List[str]) -> Tuple[nx.DiGraph, List[Set[str]]]:\n",
    "    kg = nx.DiGraph()\n",
    "    chunk_entities: List[Set[str]] = []\n",
    "\n",
    "    for i, chunk in enumerate(dataset):\n",
    "        terms = extract_terms_offline(chunk, top_k=TOP_K_TERMS_PER_CHUNK)\n",
    "        ent_set = set(terms)\n",
    "        chunk_entities.append(ent_set)\n",
    "\n",
    "        for t in ent_set:\n",
    "            if not kg.has_node(t):\n",
    "                kg.add_node(t)\n",
    "\n",
    "        # co-occurrence edges (ordered terms)\n",
    "        ordered = terms\n",
    "        for a_idx, a in enumerate(ordered):\n",
    "            for b_idx in range(a_idx+1, min(len(ordered), a_idx+1+COOC_WINDOW)):\n",
    "                b = ordered[b_idx]\n",
    "                if a == b:\n",
    "                    continue\n",
    "                # directional edge with relation label\n",
    "                if kg.has_edge(a, b):\n",
    "                    kg[a][b][\"weight\"] = kg[a][b].get(\"weight\", 1) + 1\n",
    "                else:\n",
    "                    kg.add_edge(a, b, relation=\"co_occurs_with\", weight=1, source_chunk=i)\n",
    "\n",
    "        if (i+1) % 200 == 0:\n",
    "            logger.info(f\"KG progress {i+1}/{len(dataset)} | nodes={kg.number_of_nodes()} edges={kg.number_of_edges()}\")\n",
    "\n",
    "    return kg, chunk_entities\n",
    "\n",
    "# ====== RUN OFFLINE BUILD ======\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# logger.info(\"Load JSON -> passages\")\n",
    "# doc = load_legal_json(JSON_PATH)\n",
    "# passages = flatten_to_passages(doc)\n",
    "\n",
    "# os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# Load t·∫•t c·∫£ JSON files\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(\"LOADING MULTIPLE JSON FILES\")\n",
    "logger.info(\"=\"*60)\n",
    "passages, file_metadata = load_all_json_files(JSON_FILES)\n",
    "\n",
    "if not passages:\n",
    "    raise ValueError(\"‚ùå No passages loaded! Check JSON_FILES paths.\")\n",
    "\n",
    "logger.info(\"\\nChunking\")\n",
    "dataset = make_chunks(passages)\n",
    "logger.info(f\"Total chunks = {len(dataset)}\")\n",
    "\n",
    "logger.info(\"\\nEmbedding + FAISS\")\n",
    "embeddings = embed_texts(dataset, batch_size=BATCH_EMBED)\n",
    "index = build_faiss(embeddings)\n",
    "logger.info(f\"FAISS ntotal = {index.ntotal}\")\n",
    "\n",
    "logger.info(\"\\nBuild KG + chunk_entities (NO LLM)\")\n",
    "kg, chunk_entities = build_kg_and_chunk_entities_no_llm(dataset)\n",
    "\n",
    "# ====== SAVE ARTIFACTS ======\n",
    "with open(os.path.join(CACHE_DIR, \"chunks.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset, f, ensure_ascii=False)\n",
    "\n",
    "faiss.write_index(index, os.path.join(CACHE_DIR, \"faiss.index\"))\n",
    "np.save(os.path.join(CACHE_DIR, \"embeddings.npy\"), embeddings)\n",
    "\n",
    "import pickle\n",
    "with open(os.path.join(CACHE_DIR, \"kg.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(kg, f)\n",
    "\n",
    "with open(os.path.join(CACHE_DIR, \"chunk_entities.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump([list(s) for s in chunk_entities], f, ensure_ascii=False)\n",
    "\n",
    "meta = {\n",
    "    \"created_at\": datetime.now().isoformat(),\n",
    "    \"embedding_model\": EMBED_MODEL,\n",
    "    \"num_chunks\": len(dataset),\n",
    "    \"kg_nodes\": kg.number_of_nodes(),\n",
    "    \"kg_edges\": kg.number_of_edges(),\n",
    "    \"build_mode\": \"NO_LLM\",\n",
    "    \"source_files\": file_metadata[\"files\"],\n",
    "    \"passages_per_file\": file_metadata[\"passages_per_file\"],\n",
    "}\n",
    "with open(os.path.join(CACHE_DIR, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "logger.info(f\"\\n{'='*60}\")\n",
    "logger.info(f\"‚úÖ DONE. Saved artifacts to: {CACHE_DIR}\")\n",
    "logger.info(f\"   Files processed: {len(file_metadata['files'])}\")\n",
    "logger.info(f\"   Total chunks: {len(dataset)}\")\n",
    "logger.info(f\"   KG nodes: {kg.number_of_nodes()}\")\n",
    "logger.info(f\"   KG edges: {kg.number_of_edges()}\")\n",
    "logger.info(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15aec9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: intfloat/multilingual-e5-base\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading artifacts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/multilingual-e5-base/835193815a3936a24a0ee7dc9e3d48c1fbb19c55/modules.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/config_sentence_transformers.json \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/config_sentence_transformers.json \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/multilingual-e5-base/835193815a3936a24a0ee7dc9e3d48c1fbb19c55/README.md \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/multilingual-e5-base/835193815a3936a24a0ee7dc9e3d48c1fbb19c55/modules.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/sentence_bert_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/multilingual-e5-base/835193815a3936a24a0ee7dc9e3d48c1fbb19c55/sentence_bert_config.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/adapter_config.json \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/multilingual-e5-base/835193815a3936a24a0ee7dc9e3d48c1fbb19c55/config.json \"HTTP/1.1 200 OK\"\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199/199 [00:00<00:00, 861.47it/s, Materializing param=pooler.dense.weight]                               \n",
      "XLMRobertaModel LOAD REPORT from: intfloat/multilingual-e5-base\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/multilingual-e5-base/835193815a3936a24a0ee7dc9e3d48c1fbb19c55/config.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/multilingual-e5-base/835193815a3936a24a0ee7dc9e3d48c1fbb19c55/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/intfloat/multilingual-e5-base/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/intfloat/multilingual-e5-base/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/intfloat/multilingual-e5-base \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/1_Pooling/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/multilingual-e5-base/835193815a3936a24a0ee7dc9e3d48c1fbb19c55/1_Pooling%2Fconfig.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/intfloat/multilingual-e5-base \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Artifacts loaded\n",
      "   Chunks: 3488\n",
      "   KG nodes: 4724 | KG edges: 18778\n",
      "   Source files: ['thongtu80.json', 'thongtu78.json', 'nghidinh123.json', 'nghidinh125.json', 'GTGT2008.json', 'QLT2019.json']\n",
      "\n",
      "‚úÖ Chatbot mode ready.\n"
     ]
    }
   ],
   "source": [
    "import os, json, re\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "from typing import List, Set\n",
    "\n",
    "CACHE_DIR = \"./tt80_artifacts_no_llm\"\n",
    "MODEL_ANSWER = \"moonshotai/kimi-k2-instruct-0905\"\n",
    "TOP_K = 6\n",
    "ALPHA = 0.85\n",
    "\n",
    "# --- FORCE RELOAD: ƒë·∫∑t th√†nh True khi mu·ªën reload artifacts m·ªõi ---\n",
    "FORCE_RELOAD = True\n",
    "\n",
    "# --- load artifacts ---\n",
    "if FORCE_RELOAD or \"BOT_READY\" not in globals():\n",
    "    print(\"üîÑ Loading artifacts...\")\n",
    "    \n",
    "    with open(os.path.join(CACHE_DIR, \"meta.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        META = json.load(f)\n",
    "\n",
    "    with open(os.path.join(CACHE_DIR, \"chunks.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        DATASET = json.load(f)\n",
    "\n",
    "    INDEX = faiss.read_index(os.path.join(CACHE_DIR, \"faiss.index\"))\n",
    "    \n",
    "    import pickle\n",
    "    with open(os.path.join(CACHE_DIR, \"kg.pkl\"), \"rb\") as f:\n",
    "        KG = pickle.load(f)\n",
    "\n",
    "    with open(os.path.join(CACHE_DIR, \"chunk_entities.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        CHUNK_ENTITIES = [set(x) for x in json.load(f)]\n",
    "\n",
    "    EMBEDDER = SentenceTransformer(META[\"embedding_model\"])\n",
    "    BOT_READY = True\n",
    "\n",
    "    print(\"‚úÖ Artifacts loaded\")\n",
    "    print(f\"   Chunks: {len(DATASET)}\")\n",
    "    print(f\"   KG nodes: {KG.number_of_nodes()} | KG edges: {KG.number_of_edges()}\")\n",
    "    print(f\"   Source files: {META.get('source_files', ['unknown'])}\")\n",
    "\n",
    "# --- LLM client ---\n",
    "if not os.getenv(\"NVAPI_KEY\"):\n",
    "    raise RuntimeError(\"Missing NVAPI_KEY env var.\")\n",
    "client = OpenAI(base_url=\"https://integrate.api.nvidia.com/v1\", api_key=os.getenv(\"NVAPI_KEY\"))\n",
    "\n",
    "def llm_answer(prompt: str, temperature=0.2, max_tokens=450) -> str:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=MODEL_ANSWER,\n",
    "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "        temperature=temperature,\n",
    "        top_p=0.9,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "# --- fast keyword/entity extraction without LLM ---\n",
    "STOP = set([\"l√†\", \"v√†\", \"c·ªßa\", \"theo\", \"ƒë·ªëi\", \"v·ªõi\", \"nh∆∞\", \"th·∫ø\", \"n√†o\", \"g√¨\", \"bao\", \"g·ªìm\", \"quy\", \"ƒë·ªãnh\", \"h∆∞·ªõng\", \"d·∫´n\", \"kh√¥ng\", \"c√≥\", \"m·ªôt\", \"c√°c\", \"ƒë∆∞·ª£c\"])\n",
    "\n",
    "def fast_terms(query: str) -> Set[str]:\n",
    "    q = query.lower()\n",
    "    quoted = re.findall(r\"‚Äú([^‚Äù]+)‚Äù|\\\"([^\\\"]+)\\\"\", query)\n",
    "    terms = set()\n",
    "    for a,b in quoted:\n",
    "        s = (a or b).strip().lower()\n",
    "        if s:\n",
    "            terms.add(s)\n",
    "    words = re.findall(r\"[a-zA-Z√Ä-·ªπ0-9_]+\", q)\n",
    "    for w in words:\n",
    "        if len(w) >= 3 and w not in STOP:\n",
    "            terms.add(w)\n",
    "    return terms\n",
    "\n",
    "def semantic_search(query: str, top_k: int):\n",
    "    q_vec = EMBEDDER.encode([f\"query: {query}\"], normalize_embeddings=True).astype(\"float32\")\n",
    "    D, I = INDEX.search(q_vec, top_k)\n",
    "    return [(int(I[0][j]), float(D[0][j])) for j in range(top_k)]\n",
    "\n",
    "def graph_scores(query_terms: Set[str]) -> np.ndarray:\n",
    "    scores = np.zeros(len(DATASET), dtype=float)\n",
    "    if not query_terms:\n",
    "        return scores\n",
    "    for i, ents in enumerate(CHUNK_ENTITIES):\n",
    "        hit = 0\n",
    "        for t in query_terms:\n",
    "            for e in ents:\n",
    "                if t in e.lower():\n",
    "                    hit += 1\n",
    "                    break\n",
    "        scores[i] = hit\n",
    "    if scores.max() > 0:\n",
    "        scores = scores / (scores.max() + 1e-12)\n",
    "    return scores\n",
    "\n",
    "def kg_facts_from_terms(query_terms: Set[str], limit=10) -> List[str]:\n",
    "    facts = []\n",
    "    for t in list(query_terms)[:8]:\n",
    "        cand = [n for n in KG.nodes() if t in str(n).lower()]\n",
    "        for node in cand[:2]:\n",
    "            for nb in list(KG.successors(node))[:3]:\n",
    "                rel = KG.get_edge_data(node, nb).get(\"relation\", \"related_to\")\n",
    "                facts.append(f\"{node} --{rel}--> {nb}\")\n",
    "                if len(facts) >= limit:\n",
    "                    return facts\n",
    "    return facts\n",
    "\n",
    "def retrieve_context(query: str, top_k=TOP_K, alpha=ALPHA):\n",
    "    sem = semantic_search(query, top_k=top_k*2)\n",
    "    terms = fast_terms(query)\n",
    "    gs = graph_scores(terms)\n",
    "\n",
    "    combined = []\n",
    "    for idx, sem_score in sem:\n",
    "        g = float(gs[idx])\n",
    "        combined.append((idx, alpha*sem_score + (1-alpha)*g))\n",
    "    combined.sort(key=lambda x: x[1], reverse=True)\n",
    "    idxs = [i for i,_ in combined[:top_k]]\n",
    "\n",
    "    ctx = \"\\n\\n\".join([DATASET[i] for i in idxs])\n",
    "    facts = kg_facts_from_terms(terms, limit=10)\n",
    "    if facts:\n",
    "        ctx += \"\\n\\nKG_FACTS:\\n\" + \"\\n\".join(facts)\n",
    "    return ctx, idxs\n",
    "\n",
    "def ask(query: str):\n",
    "    ctx, idxs = retrieve_context(query)\n",
    "    prompt = f\"\"\"\n",
    "B·∫°n l√† tr·ª£ l√Ω h·ªèi ƒë√°p lu·∫≠t Vi·ªát Nam.\n",
    "\n",
    "s·ª≠ d·ª•ng nh·ªØng ki·∫øn th·ª©c t·ª´ knowledge graph, kh√¥ng t·ª± b·ªãa ƒë·∫∑t nh√©\n",
    "\n",
    "h∆°n n·ªØa, nh·ªØng ki·∫øn th·ª©c c√≥ trong CONTEXT r·∫•t quan tr·ªçng, ph·∫£i d√πng ƒë·∫øn ch√∫ng ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. N√™n h√£y d√πng h·∫øt ƒë·ªÉ t√≥m t·∫Øt ƒë·ªÉ tr·∫£ l·ªùi\n",
    "\n",
    "CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n CONTEXT. Kh√¥ng b·ªãa.\n",
    "N·∫øu CONTEXT kh√¥ng c√≥ th√¥ng tin, tr·∫£ l·ªùi ƒë√∫ng 1 d√≤ng:\n",
    "no relevant information found\n",
    "\n",
    "CONTEXT:\n",
    "{ctx}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "    ans = llm_answer(prompt, temperature=0.2, max_tokens=450)\n",
    "    return ans, idxs\n",
    "\n",
    "print(\"\\n‚úÖ Chatbot mode ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4db2e73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GraphRAG Engine v2 loaded!\n",
      "   - get_kg_expanded_terms(): KG traversal ƒë·ªÉ expand query terms\n",
      "   - graph_scores_v2(): weighted scoring v·ªõi expanded terms\n",
      "   - retrieve_context_graphrag(): full GraphRAG pipeline\n",
      "   - ask_graphrag(): Q&A v·ªõi GraphRAG\n",
      "   - compare_retrieval(): so s√°nh v1 vs v2\n",
      "\n",
      "üß™ Testing v·ªõi c√¢u h·ªèi: Thu·∫ø gi√° tr·ªã gia tƒÉng (GTGT) ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a nh∆∞ th·∫ø n√†o\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 39.90it/s]\n",
      "INFO:httpx:HTTP Request: POST https://integrate.api.nvidia.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù ANSWER:\n",
      "no relevant information found\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: GraphRAG Engine v2 - Full Implementation\n",
    "\n",
    "from typing import Dict, List, Set, Tuple\n",
    "\n",
    "# ============================================================\n",
    "# GRAPHRAG ENGINE V2 - v·ªõi KG Expansion cho Retrieval\n",
    "# ============================================================\n",
    "\n",
    "def get_kg_expanded_terms(seed_terms: Set[str], hops: int = 2, max_neighbors_per_node: int = 5) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    T·ª´ seed_terms, ƒëi qua KG ƒë·ªÉ l·∫•y th√™m related terms v·ªõi decay weight.\n",
    "    Returns: {term: weight} ‚Äî weight c√†ng cao = c√†ng relevant\n",
    "    \"\"\"\n",
    "    expanded = {}\n",
    "    \n",
    "    # seed terms c√≥ weight = 1.0\n",
    "    for t in seed_terms:\n",
    "        expanded[t.lower()] = 1.0\n",
    "    \n",
    "    # BFS expansion v·ªõi decay\n",
    "    frontier = list(seed_terms)\n",
    "    visited = set(t.lower() for t in seed_terms)\n",
    "    \n",
    "    for hop in range(1, hops + 1):\n",
    "        decay = 1.0 / (hop + 1)  # hop 1 ‚Üí 0.5, hop 2 ‚Üí 0.33\n",
    "        next_frontier = []\n",
    "        \n",
    "        for term in frontier:\n",
    "            # t√¨m nodes trong KG match term\n",
    "            matching_nodes = [n for n in KG.nodes() if term.lower() in str(n).lower()]\n",
    "            \n",
    "            for node in matching_nodes[:3]:  # gi·ªõi h·∫°n nodes per term\n",
    "                # l·∫•y neighbors (c·∫£ successors v√† predecessors)\n",
    "                neighbors = list(KG.successors(node)) + list(KG.predecessors(node))\n",
    "                \n",
    "                # sort by edge weight n·∫øu c√≥\n",
    "                weighted_neighbors = []\n",
    "                for nb in neighbors:\n",
    "                    edge_data = KG.get_edge_data(node, nb) or KG.get_edge_data(nb, node) or {}\n",
    "                    w = edge_data.get(\"weight\", 1)\n",
    "                    weighted_neighbors.append((nb, w))\n",
    "                \n",
    "                weighted_neighbors.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                for nb, edge_weight in weighted_neighbors[:max_neighbors_per_node]:\n",
    "                    nb_lower = str(nb).lower()\n",
    "                    if nb_lower not in visited:\n",
    "                        visited.add(nb_lower)\n",
    "                        next_frontier.append(str(nb))\n",
    "                        # weight = decay * normalized edge weight\n",
    "                        score = decay * min(edge_weight / 5.0, 1.0)\n",
    "                        expanded[nb_lower] = max(expanded.get(nb_lower, 0), score)\n",
    "        \n",
    "        frontier = next_frontier\n",
    "    \n",
    "    return expanded\n",
    "\n",
    "\n",
    "def graph_scores_v2(expanded_terms: Dict[str, float]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ch·∫•m ƒëi·ªÉm chunks d·ª±a tr√™n expanded terms t·ª´ KG traversal.\n",
    "    Weighted matching thay v√¨ binary.\n",
    "    \"\"\"\n",
    "    scores = np.zeros(len(DATASET), dtype=float)\n",
    "    if not expanded_terms:\n",
    "        return scores\n",
    "    \n",
    "    for i, ents in enumerate(CHUNK_ENTITIES):\n",
    "        chunk_score = 0.0\n",
    "        for e in ents:\n",
    "            e_lower = e.lower()\n",
    "            # check if any expanded term matches\n",
    "            for term, weight in expanded_terms.items():\n",
    "                if term in e_lower or e_lower in term:\n",
    "                    chunk_score += weight\n",
    "                    break  # m·ªói entity ch·ªâ count 1 l·∫ßn\n",
    "        scores[i] = chunk_score\n",
    "    \n",
    "    # normalize\n",
    "    if scores.max() > 0:\n",
    "        scores = scores / (scores.max() + 1e-12)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "def kg_facts_from_expanded(expanded_terms: Dict[str, float], limit: int = 12) -> List[str]:\n",
    "    \"\"\"\n",
    "    L·∫•y facts t·ª´ KG d·ª±a tr√™n expanded terms, ∆∞u ti√™n high-weight terms.\n",
    "    \"\"\"\n",
    "    facts = []\n",
    "    seen_edges = set()\n",
    "    \n",
    "    # sort terms by weight\n",
    "    sorted_terms = sorted(expanded_terms.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for term, weight in sorted_terms[:10]:\n",
    "        if len(facts) >= limit:\n",
    "            break\n",
    "        \n",
    "        matching_nodes = [n for n in KG.nodes() if term in str(n).lower()]\n",
    "        \n",
    "        for node in matching_nodes[:2]:\n",
    "            for nb in list(KG.successors(node))[:3]:\n",
    "                edge_key = (str(node), str(nb))\n",
    "                if edge_key in seen_edges:\n",
    "                    continue\n",
    "                seen_edges.add(edge_key)\n",
    "                \n",
    "                edge_data = KG.get_edge_data(node, nb) or {}\n",
    "                rel = edge_data.get(\"relation\", \"related_to\")\n",
    "                edge_weight = edge_data.get(\"weight\", 1)\n",
    "                \n",
    "                # ch·ªâ l·∫•y edges c√≥ weight >= 2 (co-occur nhi·ªÅu l·∫ßn)\n",
    "                if edge_weight >= 2:\n",
    "                    facts.append(f\"{node} --{rel}[w={edge_weight}]--> {nb}\")\n",
    "                \n",
    "                if len(facts) >= limit:\n",
    "                    return facts\n",
    "    \n",
    "    return facts\n",
    "\n",
    "\n",
    "def retrieve_context_graphrag(query: str, top_k: int = TOP_K, alpha: float = ALPHA, \n",
    "                               kg_hops: int = 2, kg_weight: float = 0.3) -> Tuple[str, List[int], Dict]:\n",
    "    \"\"\"\n",
    "    GraphRAG retrieval v·ªõi KG expansion.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Extract seed terms t·ª´ query\n",
    "    2. Expand terms qua KG (multi-hop traversal)\n",
    "    3. Semantic search (FAISS)\n",
    "    4. Graph-based scoring v·ªõi expanded terms\n",
    "    5. Hybrid rerank: alpha*semantic + (1-alpha)*graph\n",
    "    6. Inject KG facts v√†o context\n",
    "    \n",
    "    Returns: (context_str, chunk_indices, debug_info)\n",
    "    \"\"\"\n",
    "    # Step 1: seed terms\n",
    "    seed_terms = fast_terms(query)\n",
    "    \n",
    "    # Step 2: KG expansion\n",
    "    expanded_terms = get_kg_expanded_terms(seed_terms, hops=kg_hops)\n",
    "    \n",
    "    # Step 3: semantic search\n",
    "    sem_results = semantic_search(query, top_k=top_k * 3)  # l·∫•y nhi·ªÅu h∆°n ƒë·ªÉ rerank\n",
    "    \n",
    "    # Step 4: graph scoring v·ªõi expanded terms\n",
    "    gs = graph_scores_v2(expanded_terms)\n",
    "    \n",
    "    # Step 5: hybrid rerank\n",
    "    combined = []\n",
    "    for idx, sem_score in sem_results:\n",
    "        g_score = float(gs[idx])\n",
    "        # weighted combination\n",
    "        final_score = alpha * sem_score + (1 - alpha) * g_score\n",
    "        combined.append((idx, final_score, sem_score, g_score))\n",
    "    \n",
    "    combined.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_chunks = combined[:top_k]\n",
    "    idxs = [i for i, _, _, _ in top_chunks]\n",
    "    \n",
    "    # Step 6: build context\n",
    "    ctx_parts = [DATASET[i] for i in idxs]\n",
    "    ctx = \"\\n\\n---\\n\\n\".join(ctx_parts)\n",
    "    \n",
    "    # KG facts t·ª´ expanded terms (filtered by weight)\n",
    "    facts = kg_facts_from_expanded(expanded_terms, limit=10)\n",
    "    if facts:\n",
    "        ctx += \"\\n\\n===KG_FACTS===\\n\" + \"\\n\".join(facts)\n",
    "    \n",
    "    # debug info\n",
    "    debug = {\n",
    "        \"seed_terms\": list(seed_terms),\n",
    "        \"expanded_terms_count\": len(expanded_terms),\n",
    "        \"top_expanded\": sorted(expanded_terms.items(), key=lambda x: x[1], reverse=True)[:15],\n",
    "        \"chunk_scores\": [(i, f\"sem={s:.3f} graph={g:.3f} final={f:.3f}\") \n",
    "                         for i, f, s, g in top_chunks]\n",
    "    }\n",
    "    \n",
    "    return ctx, idxs, debug\n",
    "\n",
    "\n",
    "def ask_graphrag(query: str, verbose: bool = False):\n",
    "    \"\"\"\n",
    "    GraphRAG-powered Q&A.\n",
    "    \"\"\"\n",
    "    ctx, idxs, debug = retrieve_context_graphrag(query)\n",
    "    \n",
    "    prompt = f\"\"\"B·∫°n l√† tr·ª£ l√Ω h·ªèi ƒë√°p lu·∫≠t Vi·ªát Nam.\n",
    "\n",
    "B·∫°n l√† tr·ª£ l√Ω h·ªèi ƒë√°p lu·∫≠t Vi·ªát Nam.\n",
    "\n",
    "s·ª≠ d·ª•ng nh·ªØng ki·∫øn th·ª©c t·ª´ knowledge graph, kh√¥ng t·ª± b·ªãa ƒë·∫∑t nh√©\n",
    "\n",
    "h∆°n n·ªØa, nh·ªØng ki·∫øn th·ª©c c√≥ trong CONTEXT r·∫•t quan tr·ªçng, ph·∫£i d√πng ƒë·∫øn ch√∫ng ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. N√™n h√£y d√πng h·∫øt ƒë·ªÉ t√≥m t·∫Øt ƒë·ªÉ tr·∫£ l·ªùi\n",
    "\n",
    "CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n CONTEXT. Kh√¥ng b·ªãa.\n",
    "N·∫øu CONTEXT kh√¥ng c√≥ th√¥ng tin, tr·∫£ l·ªùi ƒë√∫ng 1 d√≤ng:\n",
    "no relevant information found\n",
    "\n",
    "CONTEXT:\n",
    "{ctx}\n",
    "\n",
    "C√ÇU H·ªéI:\n",
    "{query}\n",
    "\n",
    "TR·∫¢ L·ªúI:\"\"\"\n",
    "\n",
    "    ans = llm_answer(prompt, temperature=0.2, max_tokens=600)\n",
    "    \n",
    "    # if verbose:\n",
    "    #     print(\"\\n\" + \"=\"*60)\n",
    "    #     print(\"üîç DEBUG INFO:\")\n",
    "    #     print(f\"   Seed terms: {debug['seed_terms']}\")\n",
    "    #     print(f\"   Expanded terms count: {debug['expanded_terms_count']}\")\n",
    "    #     print(f\"   Top expanded: {[t for t,_ in debug['top_expanded'][:8]]}\")\n",
    "    #     print(\"   Chunk scores:\")\n",
    "    #     for chunk_id, score_str in debug['chunk_scores']:\n",
    "    #         print(f\"      Chunk {chunk_id}: {score_str}\")\n",
    "    #     print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return ans, idxs, debug\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# So s√°nh v1 (c≈©) vs v2 (GraphRAG)\n",
    "# ============================================================\n",
    "\n",
    "def compare_retrieval(query: str):\n",
    "    \"\"\"So s√°nh k·∫øt qu·∫£ gi·ªØa retrieve_context (v1) v√† retrieve_context_graphrag (v2)\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"QUERY: {query}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # V1 - Original\n",
    "    ctx_v1, idxs_v1 = retrieve_context(query)\n",
    "    \n",
    "    # V2 - GraphRAG\n",
    "    ctx_v2, idxs_v2, debug = retrieve_context_graphrag(query)\n",
    "    \n",
    "    print(\"\\nüìå V1 (Original RAG) - Chunks retrieved:\", idxs_v1)\n",
    "    print(\"üìå V2 (GraphRAG)     - Chunks retrieved:\", idxs_v2)\n",
    "    \n",
    "    # Overlap\n",
    "    overlap = set(idxs_v1) & set(idxs_v2)\n",
    "    only_v1 = set(idxs_v1) - set(idxs_v2)\n",
    "    only_v2 = set(idxs_v2) - set(idxs_v1)\n",
    "    \n",
    "    print(f\"\\nüîÑ Overlap: {len(overlap)} chunks - {overlap}\")\n",
    "    print(f\"‚ûï Only in V1: {only_v1}\")\n",
    "    print(f\"‚ûï Only in V2 (t·ª´ KG expansion): {only_v2}\")\n",
    "    \n",
    "    print(f\"\\nüß† KG Expansion:\")\n",
    "    print(f\"   Seed terms: {debug['seed_terms']}\")\n",
    "    print(f\"   Expanded to: {debug['expanded_terms_count']} terms\")\n",
    "    print(f\"   Top expanded: {[f'{t}({w:.2f})' for t,w in debug['top_expanded'][:10]]}\")\n",
    "    \n",
    "    return idxs_v1, idxs_v2, debug\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# TEST\n",
    "# ============================================================\n",
    "\n",
    "print(\"‚úÖ GraphRAG Engine v2 loaded!\")\n",
    "print(\"   - get_kg_expanded_terms(): KG traversal ƒë·ªÉ expand query terms\")\n",
    "print(\"   - graph_scores_v2(): weighted scoring v·ªõi expanded terms\")\n",
    "print(\"   - retrieve_context_graphrag(): full GraphRAG pipeline\")\n",
    "print(\"   - ask_graphrag(): Q&A v·ªõi GraphRAG\")\n",
    "print(\"   - compare_retrieval(): so s√°nh v1 vs v2\")\n",
    "print()\n",
    "\n",
    "# Quick test\n",
    "test_question = \"Thu·∫ø gi√° tr·ªã gia tƒÉng (GTGT) ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a nh∆∞ th·∫ø n√†o\"\n",
    "print(\"üß™ Testing v·ªõi c√¢u h·ªèi:\", test_question)\n",
    "print()\n",
    "\n",
    "# answer, chunks, debug = ask_graphrag(test_question, verbose=True)\n",
    "answer, chunks, debug = ask_graphrag(test_question, verbose=True)\n",
    "print(\"üìù ANSWER:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f4837e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GraphRAG Engine v2 loaded!\n",
      "   - get_kg_expanded_terms(): KG traversal ƒë·ªÉ expand query terms\n",
      "   - graph_scores_v2(): weighted scoring v·ªõi expanded terms\n",
      "   - retrieve_context_graphrag(): full GraphRAG pipeline\n",
      "   - ask_graphrag(): Q&A v·ªõi GraphRAG\n",
      "   - compare_retrieval(): so s√°nh v1 vs v2\n",
      "\n",
      "üß™ Testing v·ªõi c√¢u h·ªèi: Thu·∫ø gi√° tr·ªã gia tƒÉng (GTGT) ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a nh∆∞ th·∫ø n√†o?\n",
      "\n",
      "üìä SO S√ÅNH V1 vs V2:\n",
      "======================================================================\n",
      "QUERY: Thu·∫ø gi√° tr·ªã gia tƒÉng (GTGT) ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a nh∆∞ th·∫ø n√†o?\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 38.70it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 38.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå V1 (Original RAG) - Chunks retrieved: [2332, 2334, 2371, 2400, 2418, 2374]\n",
      "üìå V2 (GraphRAG)     - Chunks retrieved: [2371, 1181, 2331, 2418, 2360, 2374]\n",
      "\n",
      "üîÑ Overlap: 3 chunks - {2418, 2371, 2374}\n",
      "‚ûï Only in V1: {2400, 2332, 2334}\n",
      "‚ûï Only in V2 (t·ª´ KG expansion): {2360, 2331, 1181}\n",
      "\n",
      "üß† KG Expansion:\n",
      "   Seed terms: ['gi√°', 'tr·ªã', 'thu·∫ø', 'tƒÉng', 'gia', 'nghƒ©a', 'gtgt']\n",
      "   Expanded to: 190 terms\n",
      "   Top expanded: ['gi√°(1.00)', 'tr·ªã(1.00)', 'thu·∫ø(1.00)', 'tƒÉng(1.00)', 'gia(1.00)', 'nghƒ©a(1.00)', 'gtgt(1.00)', 'thu·∫ø gi√°(0.50)', 'thu·∫ø gi√° tr·ªã(0.50)', 'thu·∫ø ƒëi·ªÅu(0.50)']\n",
      "\n",
      "üìÑ Context lengths: V1=4186 chars | V2=4238 chars\n",
      "\n",
      "======================================================================\n",
      "üìù GRAPHRAG ANSWER:\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 42.05it/s]\n",
      "INFO:httpx:HTTP Request: POST https://integrate.api.nvidia.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîç DEBUG INFO:\n",
      "   Seed terms: ['gi√°', 'tr·ªã', 'thu·∫ø', 'tƒÉng', 'gia', 'nghƒ©a', 'gtgt']\n",
      "   Expanded terms count: 190\n",
      "   Top expanded: ['gi√°', 'tr·ªã', 'thu·∫ø', 'tƒÉng', 'gia', 'nghƒ©a', 'gtgt', 'thu·∫ø gi√°']\n",
      "   Chunk scores:\n",
      "      Chunk 2371: sem=0.849 graph=0.917 final=0.859\n",
      "      Chunk 1181: sem=0.843 graph=0.935 final=0.857\n",
      "      Chunk 2331: sem=0.841 graph=0.944 final=0.857\n",
      "      Chunk 2418: sem=0.849 graph=0.900 final=0.856\n",
      "      Chunk 2360: sem=0.844 graph=0.796 final=0.837\n",
      "      Chunk 2374: sem=0.847 graph=0.733 final=0.830\n",
      "============================================================\n",
      "\n",
      "Thu·∫ø gi√° tr·ªã gia tƒÉng (GTGT) l√† lo·∫°i thu·∫ø ti√™u d√πng ƒë∆∞·ª£c t√≠nh tr√™n gi√° tr·ªã tƒÉng th√™m c·ªßa h√†ng ho√°, d·ªãch v·ª• ph√°t sinh trong qu√° tr√¨nh s·∫£n xu·∫•t, l∆∞u th√¥ng ƒë·∫øn ti√™u d√πng; cƒÉn c·ª© t√≠nh thu·∫ø g·ªìm gi√° t√≠nh thu·∫ø v√† thu·∫ø su·∫•t (ƒêi·ªÅu 6 Lu·∫≠t Thu·∫ø GTGT 2008).\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: GraphRAG Engine v2 - Full Implementation (FIXED)\n",
    "\n",
    "from typing import Dict, List, Set, Tuple\n",
    "\n",
    "# ============================================================\n",
    "# GRAPHRAG ENGINE V2 - v·ªõi KG Expansion cho Retrieval\n",
    "# ============================================================\n",
    "\n",
    "def get_kg_expanded_terms(seed_terms: Set[str], hops: int = 2, max_neighbors_per_node: int = 5) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    T·ª´ seed_terms, ƒëi qua KG ƒë·ªÉ l·∫•y th√™m related terms v·ªõi decay weight.\n",
    "    Returns: {term: weight} ‚Äî weight c√†ng cao = c√†ng relevant\n",
    "    \"\"\"\n",
    "    expanded = {}\n",
    "    \n",
    "    # seed terms c√≥ weight = 1.0\n",
    "    for t in seed_terms:\n",
    "        expanded[t.lower()] = 1.0\n",
    "    \n",
    "    # BFS expansion v·ªõi decay\n",
    "    frontier = list(seed_terms)\n",
    "    visited = set(t.lower() for t in seed_terms)\n",
    "    \n",
    "    for hop in range(1, hops + 1):\n",
    "        decay = 1.0 / (hop + 1)  # hop 1 ‚Üí 0.5, hop 2 ‚Üí 0.33\n",
    "        next_frontier = []\n",
    "        \n",
    "        for term in frontier:\n",
    "            # t√¨m nodes trong KG match term\n",
    "            matching_nodes = [n for n in KG.nodes() if term.lower() in str(n).lower()]\n",
    "            \n",
    "            for node in matching_nodes[:3]:  # gi·ªõi h·∫°n nodes per term\n",
    "                # l·∫•y neighbors (c·∫£ successors v√† predecessors)\n",
    "                neighbors = list(KG.successors(node)) + list(KG.predecessors(node))\n",
    "                \n",
    "                # sort by edge weight n·∫øu c√≥\n",
    "                weighted_neighbors = []\n",
    "                for nb in neighbors:\n",
    "                    edge_data = KG.get_edge_data(node, nb) or KG.get_edge_data(nb, node) or {}\n",
    "                    w = edge_data.get(\"weight\", 1)\n",
    "                    weighted_neighbors.append((nb, w))\n",
    "                \n",
    "                weighted_neighbors.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                for nb, edge_weight in weighted_neighbors[:max_neighbors_per_node]:\n",
    "                    nb_lower = str(nb).lower()\n",
    "                    if nb_lower not in visited:\n",
    "                        visited.add(nb_lower)\n",
    "                        next_frontier.append(str(nb))\n",
    "                        # weight = decay * normalized edge weight\n",
    "                        score = decay * min(edge_weight / 5.0, 1.0)\n",
    "                        expanded[nb_lower] = max(expanded.get(nb_lower, 0), score)\n",
    "        \n",
    "        frontier = next_frontier\n",
    "    \n",
    "    return expanded\n",
    "\n",
    "\n",
    "def graph_scores_v2(expanded_terms: Dict[str, float]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ch·∫•m ƒëi·ªÉm chunks d·ª±a tr√™n expanded terms t·ª´ KG traversal.\n",
    "    Weighted matching thay v√¨ binary.\n",
    "    \"\"\"\n",
    "    scores = np.zeros(len(DATASET), dtype=float)\n",
    "    if not expanded_terms:\n",
    "        return scores\n",
    "    \n",
    "    for i, ents in enumerate(CHUNK_ENTITIES):\n",
    "        chunk_score = 0.0\n",
    "        for e in ents:\n",
    "            e_lower = e.lower()\n",
    "            # check if any expanded term matches\n",
    "            for term, weight in expanded_terms.items():\n",
    "                if term in e_lower or e_lower in term:\n",
    "                    chunk_score += weight\n",
    "                    break  # m·ªói entity ch·ªâ count 1 l·∫ßn\n",
    "        scores[i] = chunk_score\n",
    "    \n",
    "    # normalize\n",
    "    if scores.max() > 0:\n",
    "        scores = scores / (scores.max() + 1e-12)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "def kg_facts_from_expanded(expanded_terms: Dict[str, float], limit: int = 12) -> List[str]:\n",
    "    \"\"\"\n",
    "    L·∫•y facts t·ª´ KG d·ª±a tr√™n expanded terms, ∆∞u ti√™n high-weight terms.\n",
    "    \"\"\"\n",
    "    facts = []\n",
    "    seen_edges = set()\n",
    "    \n",
    "    # sort terms by weight\n",
    "    sorted_terms = sorted(expanded_terms.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for term, weight in sorted_terms[:10]:\n",
    "        if len(facts) >= limit:\n",
    "            break\n",
    "        \n",
    "        matching_nodes = [n for n in KG.nodes() if term in str(n).lower()]\n",
    "        \n",
    "        for node in matching_nodes[:2]:\n",
    "            for nb in list(KG.successors(node))[:3]:\n",
    "                edge_key = (str(node), str(nb))\n",
    "                if edge_key in seen_edges:\n",
    "                    continue\n",
    "                seen_edges.add(edge_key)\n",
    "                \n",
    "                edge_data = KG.get_edge_data(node, nb) or {}\n",
    "                rel = edge_data.get(\"relation\", \"related_to\")\n",
    "                edge_weight = edge_data.get(\"weight\", 1)\n",
    "                \n",
    "                # L·∫•y t·∫•t c·∫£ edges (b·ªè filter weight >= 2)\n",
    "                facts.append(f\"{node} --{rel}[w={edge_weight}]--> {nb}\")\n",
    "                \n",
    "                if len(facts) >= limit:\n",
    "                    return facts\n",
    "    \n",
    "    return facts\n",
    "\n",
    "\n",
    "def retrieve_context_graphrag(query: str, top_k: int = TOP_K, alpha: float = ALPHA, \n",
    "                               kg_hops: int = 2) -> Tuple[str, List[int], Dict]:\n",
    "    \"\"\"\n",
    "    GraphRAG retrieval v·ªõi KG expansion.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Extract seed terms t·ª´ query\n",
    "    2. Expand terms qua KG (multi-hop traversal)\n",
    "    3. Semantic search (FAISS)\n",
    "    4. Graph-based scoring v·ªõi expanded terms\n",
    "    5. Hybrid rerank: alpha*semantic + (1-alpha)*graph\n",
    "    6. Inject KG facts v√†o context\n",
    "    \n",
    "    Returns: (context_str, chunk_indices, debug_info)\n",
    "    \"\"\"\n",
    "    # Step 1: seed terms\n",
    "    seed_terms = fast_terms(query)\n",
    "    \n",
    "    # Step 2: KG expansion\n",
    "    expanded_terms = get_kg_expanded_terms(seed_terms, hops=kg_hops)\n",
    "    \n",
    "    # Step 3: semantic search - L·∫§Y NHI·ªÄU H∆†N ƒë·ªÉ c√≥ pool rerank\n",
    "    sem_results = semantic_search(query, top_k=top_k * 3)\n",
    "    \n",
    "    # Step 4: graph scoring v·ªõi expanded terms\n",
    "    gs = graph_scores_v2(expanded_terms)\n",
    "    \n",
    "    # Step 5: hybrid rerank\n",
    "    combined = []\n",
    "    for idx, sem_score in sem_results:\n",
    "        g_score = float(gs[idx])\n",
    "        # weighted combination\n",
    "        final_score = alpha * sem_score + (1 - alpha) * g_score\n",
    "        combined.append((idx, final_score, sem_score, g_score))\n",
    "    \n",
    "    combined.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_chunks = combined[:top_k]\n",
    "    idxs = [i for i, _, _, _ in top_chunks]\n",
    "    \n",
    "    # Step 6: build context - GI·ªêNG V1: d√πng \"\\n\\n\" thay v√¨ \"---\"\n",
    "    ctx_parts = [DATASET[i] for i in idxs]\n",
    "    ctx = \"\\n\\n\".join(ctx_parts)\n",
    "    \n",
    "    # KG facts t·ª´ expanded terms\n",
    "    facts = kg_facts_from_expanded(expanded_terms, limit=10)\n",
    "    if facts:\n",
    "        ctx += \"\\n\\nKG_FACTS:\\n\" + \"\\n\".join(facts)\n",
    "    \n",
    "    # debug info\n",
    "    debug = {\n",
    "        \"seed_terms\": list(seed_terms),\n",
    "        \"expanded_terms_count\": len(expanded_terms),\n",
    "        \"top_expanded\": sorted(expanded_terms.items(), key=lambda x: x[1], reverse=True)[:15],\n",
    "        \"chunk_scores\": [(i, f\"sem={s:.3f} graph={g:.3f} final={f:.3f}\") \n",
    "                         for i, f, s, g in top_chunks]\n",
    "    }\n",
    "    \n",
    "    return ctx, idxs, debug\n",
    "\n",
    "\n",
    "def ask_graphrag(query: str, verbose: bool = False):\n",
    "    \"\"\"\n",
    "    GraphRAG-powered Q&A.\n",
    "    \"\"\"\n",
    "    ctx, idxs, debug = retrieve_context_graphrag(query)\n",
    "    \n",
    "    # PROMPT GI·ªêNG V1\n",
    "    prompt = f\"\"\"B·∫°n l√† tr·ª£ l√Ω h·ªèi ƒë√°p lu·∫≠t Vi·ªát Nam.\n",
    "\n",
    "s·ª≠ d·ª•ng nh·ªØng ki·∫øn th·ª©c t·ª´ knowledge graph, kh√¥ng t·ª± b·ªãa ƒë·∫∑t nh√©\n",
    "\n",
    "h∆°n n·ªØa, nh·ªØng ki·∫øn th·ª©c c√≥ trong CONTEXT r·∫•t quan tr·ªçng, ph·∫£i d√πng ƒë·∫øn ch√∫ng ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. N√™n h√£y d√πng h·∫øt ƒë·ªÉ t√≥m t·∫Øt ƒë·ªÉ tr·∫£ l·ªùi\n",
    "\n",
    "CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n CONTEXT. Kh√¥ng b·ªãa.\n",
    "N·∫øu CONTEXT kh√¥ng c√≥ th√¥ng tin, tr·∫£ l·ªùi ƒë√∫ng 1 d√≤ng:\n",
    "no relevant information found\n",
    "\n",
    "CONTEXT:\n",
    "{ctx}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "    ans = llm_answer(prompt, temperature=0.2, max_tokens=600)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üîç DEBUG INFO:\")\n",
    "        print(f\"   Seed terms: {debug['seed_terms']}\")\n",
    "        print(f\"   Expanded terms count: {debug['expanded_terms_count']}\")\n",
    "        print(f\"   Top expanded: {[t for t,_ in debug['top_expanded'][:8]]}\")\n",
    "        print(\"   Chunk scores:\")\n",
    "        for chunk_id, score_str in debug['chunk_scores']:\n",
    "            print(f\"      Chunk {chunk_id}: {score_str}\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return ans, idxs, debug\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# So s√°nh v1 (c≈©) vs v2 (GraphRAG)\n",
    "# ============================================================\n",
    "\n",
    "def compare_retrieval(query: str):\n",
    "    \"\"\"So s√°nh k·∫øt qu·∫£ gi·ªØa retrieve_context (v1) v√† retrieve_context_graphrag (v2)\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"QUERY: {query}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # V1 - Original\n",
    "    ctx_v1, idxs_v1 = retrieve_context(query)\n",
    "    \n",
    "    # V2 - GraphRAG\n",
    "    ctx_v2, idxs_v2, debug = retrieve_context_graphrag(query)\n",
    "    \n",
    "    print(\"\\nüìå V1 (Original RAG) - Chunks retrieved:\", idxs_v1)\n",
    "    print(\"üìå V2 (GraphRAG)     - Chunks retrieved:\", idxs_v2)\n",
    "    \n",
    "    # Overlap\n",
    "    overlap = set(idxs_v1) & set(idxs_v2)\n",
    "    only_v1 = set(idxs_v1) - set(idxs_v2)\n",
    "    only_v2 = set(idxs_v2) - set(idxs_v1)\n",
    "    \n",
    "    print(f\"\\nüîÑ Overlap: {len(overlap)} chunks - {overlap}\")\n",
    "    print(f\"‚ûï Only in V1: {only_v1}\")\n",
    "    print(f\"‚ûï Only in V2 (t·ª´ KG expansion): {only_v2}\")\n",
    "    \n",
    "    print(f\"\\nüß† KG Expansion:\")\n",
    "    print(f\"   Seed terms: {debug['seed_terms']}\")\n",
    "    print(f\"   Expanded to: {debug['expanded_terms_count']} terms\")\n",
    "    print(f\"   Top expanded: {[f'{t}({w:.2f})' for t,w in debug['top_expanded'][:10]]}\")\n",
    "    \n",
    "    # So s√°nh context\n",
    "    print(f\"\\nüìÑ Context lengths: V1={len(ctx_v1)} chars | V2={len(ctx_v2)} chars\")\n",
    "    \n",
    "    return idxs_v1, idxs_v2, debug\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# TEST\n",
    "# ============================================================\n",
    "\n",
    "print(\"‚úÖ GraphRAG Engine v2 loaded!\")\n",
    "print(\"   - get_kg_expanded_terms(): KG traversal ƒë·ªÉ expand query terms\")\n",
    "print(\"   - graph_scores_v2(): weighted scoring v·ªõi expanded terms\")\n",
    "print(\"   - retrieve_context_graphrag(): full GraphRAG pipeline\")\n",
    "print(\"   - ask_graphrag(): Q&A v·ªõi GraphRAG\")\n",
    "print(\"   - compare_retrieval(): so s√°nh v1 vs v2\")\n",
    "print()\n",
    "\n",
    "# Quick test\n",
    "test_question = \"Thu·∫ø gi√° tr·ªã gia tƒÉng (GTGT) ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a nh∆∞ th·∫ø n√†o?\"\n",
    "print(\"üß™ Testing v·ªõi c√¢u h·ªèi:\", test_question)\n",
    "print()\n",
    "\n",
    "# So s√°nh V1 vs V2 tr∆∞·ªõc\n",
    "print(\"üìä SO S√ÅNH V1 vs V2:\")\n",
    "compare_retrieval(test_question)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìù GRAPHRAG ANSWER:\")\n",
    "print(\"=\"*70)\n",
    "answer, chunks, debug = ask_graphrag(test_question, verbose=True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d536e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 35.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "Thu·∫ø gi√° tr·ªã gia tƒÉng (GTGT) ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a nh∆∞ th·∫ø n√†o?\n",
      "\n",
      "CONTEXT USED:\n",
      "[article] VƒÉn b·∫£n: Lu·∫≠t thu·∫ø gi√° tr·ªã gia tƒÉng 2008 s·ªë 13/2008/QH12 √°p d·ª•ng 2024 > Ngu·ªìn: GTGT2008.json > URL: https://thuvienphapluat.vn/van-ban/Thue-Phi-Le-Phi/Luat-thue-gia-tri-gia-tang-2008-13-2008-QH12-66934.aspx > Crawled: 2026-01-24 > Ch∆∞∆°ng 1.\n",
      "NH·ªÆNG QUY ƒê·ªäNH CHUNG > ƒêi·ªÅu 2. Thu·∫ø gi√° tr·ªã gia tƒÉng\n",
      "Thu·∫ø gi√° tr·ªã gia tƒÉng l√† thu·∫ø\n",
      "t√≠nh tr√™n gi√° tr·ªã tƒÉng th√™m c·ªßa h√†ng h√≥a, d·ªãch v·ª• ph√°t sinh trong qu√° tr√¨nh t·ª´\n",
      "s·∫£n xu·∫•t, l∆∞u th√¥ng ƒë·∫øn ti√™u d√πng.\n",
      "\n",
      "[article] VƒÉn b·∫£n: Lu·∫≠t thu·∫ø gi√° tr·ªã gia tƒÉng 2008 s·ªë 13/2008/QH12 √°p d·ª•ng 2024 > Ngu·ªìn: GTGT2008.json > URL: https://thuvienphapluat.vn/van-ban/Thue-Phi-Le-Phi/Luat-thue-gia-tri-gia-tang-2008-13-2008-QH12-66934.aspx > Crawled: 2026-01-24 > Ch∆∞∆°ng 1.\n",
      "NH·ªÆNG QUY ƒê·ªäNH CHUNG > ƒêi·ªÅu 4. Ng∆∞·ªùi n·ªôp thu·∫ø\n",
      "Ng∆∞·ªùi n·ªôp thu·∫ø gi√° tr·ªã gia tƒÉng\n",
      "l√† t·ªï ch·ª©c, c√° nh√¢n s·∫£n xu·∫•t, kinh doanh h√†ng h√≥a, d·ªãch v·ª• ch·ªãu thu·∫ø gi√° tr·ªã\n",
      "gia tƒÉng (sau ƒë√¢y g·ªçi l√† c∆° s·ªü kinh doanh) v√† t·ªï ch·ª©c, c√° nh√¢n nh·∫≠p kh·∫©u h√†ng\n",
      "h√≥a ch·ªãu thu·∫ø gi√° tr·ªã gia tƒÉng (sau ƒë√¢y g·ªçi l√† ng∆∞·ªùi nh·∫≠p kh·∫©u).\n",
      "\n",
      "[point] VƒÉn b·∫£n: Lu·∫≠t thu·∫ø gi√° tr·ªã gia tƒÉng 2008 s·ªë 13/2008/QH12 √°p d·ª•ng 2024 > Ngu·ªìn: GTGT2008.json > URL: https://thuvienphapluat.vn/van-ban/Thue-Phi-Le-Phi/Luat-thue-gia-tri-gia-tang-2008-13-2008-QH12-66934.aspx > Crawled: 2026-01-24 > Ch∆∞∆°ng 2.\n",
      "CƒÇN C·ª® V√Ä PH∆Ø∆†NG PH√ÅP\n",
      "T√çNH THU·∫æ > ƒêi·ªÅu 7. Gi√° t√≠nh thu·∫ø > 1 > k\n",
      "ƒê·ªëi v·ªõi\n",
      "h√†ng h√≥a, d·ªãch v·ª• ƒë∆∞·ª£c s·ª≠ d·ª•ng ch·ª©ng t·ª´ thanh to√°n ghi gi√° thanh to√°n l√† gi√° ƒë√£\n",
      "c√≥ thu·∫ø gi√° tr·ªã gia tƒÉng th√¨ gi√° t√≠nh thu·∫ø ƒë∆∞·ª£c x√°c ƒë·ªãnh theo c√¥ng th·ª©c sau:\n",
      "| Gi√°\n",
      "ch∆∞a c√≥ thu·∫ø gi√° tr·ªã gia tƒÉng | = | Gi√°\n",
      "thanh to√°n |\n",
      "| 1\n",
      "+ thu·∫ø su·∫•t c·ªßa h√†ng h√≥a, d·ªãch v·ª• (%) |\n",
      "\n",
      "[point] VƒÉn b·∫£n: Lu·∫≠t thu·∫ø gi√° tr·ªã gia tƒÉng 2008 s·ªë 13/2008/QH12 √°p d·ª•ng 2024 > Ngu·ªìn: GTGT2008.json > URL: https://thuvienphapluat.vn/van-ban/Thue-Phi-Le-Phi/Luat-thue-gia-tri-gia-tang-2008-13-2008-QH12-66934.aspx > Crawled: 2026-01-24 > Ch∆∞∆°ng 2.\n",
      "CƒÇN C·ª® V√Ä PH∆Ø∆†NG PH√ÅP\n",
      "T√çNH THU·∫æ > ƒêi·ªÅu 11. Ph∆∞∆°ng ph√°p t√≠nh tr·ª±c ti·∫øp tr√™n gi√° tr·ªã gia tƒÉng > 1 > b\n",
      "Gi√° tr·ªã gia tƒÉng ƒë∆∞·ª£c x√°c ƒë·ªãnh\n",
      "b·∫±ng gi√° thanh to√°n c·ªßa h√†ng h√≥a, d·ªãch v·ª• b√°n ra tr·ª´ gi√° thanh to√°n c·ªßa h√†ng\n",
      "h√≥a, d·ªãch v·ª• mua v√†o t∆∞∆°ng ·ª©ng.\n",
      "\n",
      "[point] VƒÉn b·∫£n: Lu·∫≠t thu·∫ø gi√° tr·ªã gia tƒÉng 2008 s·ªë 13/2008/QH12 √°p d·ª•ng 2024 > Ngu·ªìn: GTGT2008.json > URL: https://thuvienphapluat.vn/van-ban/Thue-Phi-Le-Phi/Luat-thue-gia-tri-gia-tang-2008-13-2008-QH12-66934.aspx > Crawled: 2026-01-24 > Ch∆∞∆°ng 3.\n",
      "KH·∫§U TR·ª™, HO√ÄN THU·∫æ > ƒêi·ªÅu 14. H√≥a ƒë∆°n, ch·ª©ng t·ª´ > 1 > a\n",
      "C∆° s·ªü kinh doanh n·ªôp thu·∫ø\n",
      "theo ph∆∞∆°ng ph√°p kh·∫•u tr·ª´ thu·∫ø s·ª≠ d·ª•ng h√≥a ƒë∆°n gi√° tr·ªã gia tƒÉng; h√≥a ƒë∆°n ph·∫£i\n",
      "ƒë∆∞·ª£c ghi ƒë·∫ßy ƒë·ªß, ƒë√∫ng n·ªôi dung quy ƒë·ªãnh, bao g·ªìm c·∫£ kho·∫£n ph·ª• thu, ph√≠ thu th√™m\n",
      "(n·∫øu c√≥). Tr∆∞·ªùng h·ª£p b√°n h√†ng h√≥a, d·ªãch v·ª• ch·ªãu thu·∫ø gi√° tr·ªã gia tƒÉng m√† tr√™n\n",
      "h√≥a ƒë∆°n gi√° tr·ªã gia tƒÉng kh√¥ng ghi kho·∫£n thu·∫ø gi√° tr·ªã gia tƒÉng th√¨ thu·∫ø gi√° tr·ªã\n",
      "gia tƒÉng ƒë·∫ßu ra ƒë∆∞·ª£c x√°c ƒë·ªãnh b·∫±ng gi√° thanh to√°n ghi tr√™n h√≥a ƒë∆°n nh√¢n v·ªõi thu·∫ø\n",
      "su·∫•t thu·∫ø gi√° tr·ªã gia tƒÉng, tr·ª´ tr∆∞·ªùng h·ª£p quy ƒë·ªãnh t·∫°i kho·∫£n 2 ƒêi·ªÅu n√†y;\n",
      "\n",
      "[clause] VƒÉn b·∫£n: Lu·∫≠t thu·∫ø gi√° tr·ªã gia tƒÉng 2008 s·ªë 13/2008/QH12 √°p d·ª•ng 2024 > Ngu·ªìn: GTGT2008.json > URL: https://thuvienphapluat.vn/van-ban/Thue-Phi-Le-Phi/Luat-thue-gia-tri-gia-tang-2008-13-2008-QH12-66934.aspx > Crawled: 2026-01-24 > Ch∆∞∆°ng 2.\n",
      "CƒÇN C·ª® V√Ä PH∆Ø∆†NG PH√ÅP\n",
      "T√çNH THU·∫æ > ƒêi·ªÅu 8. Thu·∫ø su·∫•t > 1\n",
      "M·ª©c thu·∫ø\n",
      "su·∫•t 0% √°p d·ª•ng ƒë·ªëi v·ªõi h√†ng h√≥a, d·ªãch v·ª• xu·∫•t kh·∫©u, v·∫≠n t·∫£i qu·ªëc t·∫ø v√† h√†ng\n",
      "h√≥a, d·ªãch v·ª• kh√¥ng ch·ªãu thu·∫ø gi√° tr·ªã gia tƒÉng quy ƒë·ªãnh t·∫°i ƒêi·ªÅu\n",
      "5 c·ªßa Lu·∫≠t n√†y khi xu·∫•t kh·∫©u, tr·ª´ c√°c tr∆∞·ªùng h·ª£p\n",
      "chuy·ªÉn giao c√¥ng ngh·ªá, chuy·ªÉn nh∆∞·ª£ng quy·ªÅn s·ªü h·ªØu tr√≠ tu·ªá ra n∆∞·ªõc ngo√†i; d·ªãch v·ª•\n",
      "t√°i b·∫£o hi·ªÉm ra n∆∞·ªõc ngo√†i; d·ªãch v·ª• c·∫•p t√≠n d·ª•ng, chuy·ªÉn nh∆∞·ª£ng v·ªën, d·ªãch v·ª•\n",
      "t√†i ch√≠nh ph√°t sinh; d·ªãch v·ª• b∆∞u ch√≠nh, vi·ªÖn th√¥ng; s·∫£n ph·∫©m xu·∫•t kh·∫©u l√† t√†i\n",
      "nguy√™n, kho√°ng s·∫£n khai th√°c ch∆∞a qua ch·∫ø bi·∫øn quy ƒë·ªãnh t·∫°i kho·∫£n\n",
      "23 ƒêi·ªÅu 5 c·ªßa Lu·∫≠t n√†y.\n",
      "\n",
      "KG_FACTS:\n",
      "thu·∫ø gi√° tr·ªã gia --co_occurs_with--> thu·∫ø thu\n",
      "thu·∫ø gi√° tr·ªã gia --co_occurs_with--> thu·∫ø thu nh·∫≠p\n",
      "thu·∫ø gi√° tr·ªã gia --co_occurs_with--> thu·∫ø t·∫°i\n",
      "thu·∫ø gi√° tr·ªã --co_occurs_with--> thu·∫ø gi√° tr·ªã gia\n",
      "thu·∫ø gi√° tr·ªã --co_occurs_with--> thu·∫ø thu\n",
      "thu·∫ø gi√° tr·ªã --co_occurs_with--> thu·∫ø t·∫°i\n",
      "thu·∫ø gi√° tr·ªã gia --co_occurs_with--> thu·∫ø thu\n",
      "thu·∫ø gi√° tr·ªã gia --co_occurs_with--> thu·∫ø thu nh·∫≠p\n",
      "thu·∫ø gi√° tr·ªã gia --co_occurs_with--> thu·∫ø t·∫°i\n",
      "thu·∫ø gi√° tr·ªã --co_occurs_with--> thu·∫ø gi√° tr·ªã gia\n",
      "\n",
      "ANSWER:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://integrate.api.nvidia.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu·∫ø gi√° tr·ªã gia tƒÉng l√† thu·∫ø t√≠nh tr√™n gi√° tr·ªã tƒÉng th√™m c·ªßa h√†ng h√≥a, d·ªãch v·ª• ph√°t sinh trong qu√° tr√¨nh t·ª´ s·∫£n xu·∫•t, l∆∞u th√¥ng ƒë·∫øn ti√™u d√πng.\n",
      "\n",
      "USED CHUNKS:\n",
      "[2332, 2334, 2371, 2400, 2418, 2374]\n"
     ]
    }
   ],
   "source": [
    "# question = \"Ho·∫°t ƒë·ªông th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ ƒë∆∞·ª£c hi·ªÉu nh∆∞ th·∫ø n√†o?\"\n",
    "question = \"Thu·∫ø gi√° tr·ªã gia tƒÉng (GTGT) ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a nh∆∞ th·∫ø n√†o?\"\n",
    "\n",
    "context, used_chunks = retrieve_context(question)\n",
    "\n",
    "print(\"QUESTION:\")\n",
    "print(question)\n",
    "\n",
    "print(\"\\nCONTEXT USED:\")\n",
    "print(context)\n",
    "\n",
    "print(\"\\nANSWER:\")\n",
    "print(llm_answer(f\"\"\"\n",
    "B·∫°n l√† tr·ª£ l√Ω h·ªèi ƒë√°p lu·∫≠t Vi·ªát Nam.\n",
    "\n",
    "CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n CONTEXT.\n",
    "N·∫øu kh√¥ng c√≥ th√¥ng tin, tr·∫£ l·ªùi:\n",
    "no relevant information found\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"))\n",
    "\n",
    "print(\"\\nUSED CHUNKS:\")\n",
    "print(used_chunks)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-graphrag (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
